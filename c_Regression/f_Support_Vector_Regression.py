#!/usr/bin/env python
# coding: utf-8

# ### Support Vector Regression (SVR)
# ##### ì¶œì²˜: JIYOON LEE
# - ê³¼ì í•©ì´ ë°œìƒí•˜ë©´ íšŒê·€ ê³„ìˆ˜ Wì˜ í¬ê¸°ë„ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— íšŒê·€ê³„ìˆ˜ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ëŠ” ì •ê·œí™” ë°©ë²•ì„ ì ìš©í•œë‹¤.
# <img src="./images/support_vector_regression01.png" width="450" style="margin:10px; margin-left: 0">
# - L2 ê·œì œë¥¼ ì‚¬ìš©í•˜ëŠ” ë¦¿ì§€(Ridge)ì˜ ëª©ì ì€ ì‹¤ì œê°’ê³¼ ì¶”ì •ê°’ì˜ ì°¨ì´ë¥¼ ì‘ê²Œ í•˜ë˜, íšŒê·€ê³„ìˆ˜ í¬ê¸°ë„ ì‘ê²Œ í•˜ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.
# - íŒ¨ë„í‹°ë¥¼ íšŒê·€ ê³„ìˆ˜ì— ë¶€ì—¬í•œë‹¤.
# <img src="./images/support_vector_regression02.png" width="350" style="margin:10px; margin-left: 0">
# - SVR(Support Vector Regression)ë„ L2 ê·œì œë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ëª©ì ì€ íšŒê·€ê³„ìˆ˜ í¬ê¸°ë¥¼ ì‘ê²Œ í•˜ì—¬ íšŒê·€ì‹ì„ í‰í‰í•˜ê²Œ ë§Œë“¤ë˜, ì‹¤ì œê°’ê³¼ ì¶”ì •ê°’ì˜ ì°¨ì´ë¥¼ ì‘ê²Œ í•˜ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.
# - íŒ¨ë„í‹°ë¥¼ ì†ì‹¤ í•¨ìˆ˜ì— ë¶€ì—¬í•œë‹¤.
# <img src="./images/support_vector_regression03.png" width="350" style="margin:10px; margin-left: 0">
# 
# ##### Ïµ(epsilon)-insensitive Loss function
# -  epsilon: ì ˆëŒ€ê°’ì—ì„œ ì–‘ìˆ˜ë§Œ ë‚¨ê¸´ë‹¤.
# -  SVRì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ Ïµ-insensitiveí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ SVRì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.
# > - Ïµ: íšŒê·€ì‹ ë§ˆì§„(íŠœë¸Œ)
# > - Î¾:íŠœë¸Œ ìœ„ ë°©í–¥ìœ¼ë¡œ ë²—ì–´ë‚œ ê±°ë¦¬
# > - Î¾<sup>âˆ—</sup>íŠœë¸Œ ì•„ë˜ ë°©í–¥ìœ¼ë¡œ ë²—ì–´ë‚œ ê±°ë¦¬
# <img src="./images/support_vector_regression04.png" width="300" style="margin:10px; margin-left: 0">
# <img src="./images/support_vector_regression05.png" width="500" style="margin:10px; margin-left: 0">
# - SVRì€ íšŒê·€ì‹ì´ ì¶”ì •ë˜ë©´ íšŒê·€ì‹ ìœ„ì•„ë˜ 2Ïµ(âˆ’Ïµ,Ïµ)ë§Œí¼ íŠœë¸Œë¥¼ ìƒì„±í•˜ì—¬ íšŒê·€ì„ ì— ëŒ€í•œ ìƒí•œì„ , í•˜í•œì„ ì„ ì£¼ê²Œëœë‹¤.
# ##### ğŸš© ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ìˆë‹¤ê³  ê°€ì •í•˜ë©°, ì‹¤ì œ ê°’ì„ ì™„ë²½íˆ ì¶”ì •í•˜ëŠ” ê²ƒì„ ì¶”êµ¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì ì • ë²”ìœ„(2Ïµ) ë‚´ì— ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ í—ˆìš©í•œë‹¤.
# ##### ğŸš© SVRì€ ì†ë„ê°€ ë§ì´ ëŠë¦¬ë‹¤.
# 
# ---
# ##### All Loss function hyper parameter
# <img src="./images/support_vector_regression06.png" width="550" style="margin:10px; margin-left: 0">

# ##### SVR(kernel='rbf', degree=3, gamma='scale', C=1.0, epsilon=0.1)
# - kernel: ì£¼ì–´ì§„ ë°ì´í„°ì— ì‚¬ìš©í•˜ëŠ” ì»¤ë„í•¨ìˆ˜ì— ë”°ë¼ feature spaceì˜ íŠ¹ì§•ì´ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ë°ì´í„° íŠ¹ì„±ì— ì í•©í•œ ì»¤ë„í•¨ìˆ˜ë¥¼ ê²°ì •í•œë‹¤.
# - degree: 'poly' kernelì¼ ê²½ìš°ë§Œ ì‚¬ìš©í•˜ë©°, ì–‘ìˆ˜ë§Œ ê°€ëŠ¥í•˜ê³  ë‹¤ë¥¸ kernelì—ì„œëŠ” ë¬´ì‹œëœë‹¤.
# <img src="./images/support_vector_regression07.png" width="500" style="margin:10px; margin-left: 0">
# 
# - gamma: ì»¤ë„ì˜ í­ì„ ì œì–´í•˜ê²Œ ë˜ë©°, gammaê°€ í´ìˆ˜ë¡ íšŒê·€ì„  ì»¤ë¸Œê°€ ì‹¬í•´ì§„ë‹¤.
# <img src="./images/support_vector_regression08.png" width="500" style="margin:10px; margin-left: 0">
# 
# - C: Costê°€ ì‘ì•„ì§€ë©´ ì˜ëª» ì˜ˆì¸¡í•œ ê°’ì— ëŒ€í•´, penalty ë¶€ì—¬ë¥¼ ì ê²Œ í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´ê°€ ë¬´ì‹œëœë‹¤. íšŒê·€ì‹ì´ í‰í‰í•´ì§€ë©°, ì˜ˆì¸¡ì„±ëŠ¥ë„ ê°ì†Œí•œë‹¤.
# <img src="./images/support_vector_regression09.png" width="500" style="margin:10px; margin-left: 0">
# 
# - epsilon: ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ ì˜ëª» ì˜ˆì¸¡í•œ ê°’ì„ ë§ì´ í—ˆìš©í•´ì£¼ê¸° ë•Œë¬¸ì—, support vectorì˜ ìˆ˜ë„ ê°ì†Œí•˜ê²Œ ë˜ê³ , í‰í‰í•œ íšŒê·€ì‹ì´ ë‚˜íƒ€ë‚œë‹¤.
# <img src="./images/support_vector_regression10.png" width="500" style="margin:10px; margin-left: 0">

# ##### 1ì¸ë‹¹ ê±´ê°• ë³´í—˜ ë¹„ìš©
# - age: 1ì°¨ ìˆ˜í˜œìì˜ ì—°ë ¹.
# - sex: ë³´í—˜ê³„ì•½ìì˜ ì„±ë³„(ì—¬ì„± ë˜ëŠ” ë‚¨ì„±).
# - bmi: ì²´ì§ˆëŸ‰ì§€ìˆ˜, í‚¤ ëŒ€ë¹„ ì²´ì¤‘ì„ ì¸¡ì •í•˜ëŠ” ì²™ë„.
# - children: ê±´ê°•ë³´í—˜ì˜ ì ìš©ì„ ë°›ëŠ” ìë…€ì˜ ìˆ˜ ë˜ëŠ” ë¶€ì–‘ê°€ì¡±ì˜ ìˆ˜.
# - smoker: í¡ì—° ìƒíƒœ(í¡ì—°ì ë˜ëŠ” ë¹„í¡ì—°ì).
# - region: ìˆ˜í˜œìì˜ ë¯¸êµ­ ë‚´ ê±°ì£¼ì§€ì—­(ë¶ë™, ë‚¨ë™, ë‚¨ì„œ, ë¶ì„œ).
# - charges: ê±´ê°•ë³´í—˜ì—ì„œ ì²­êµ¬í•˜ëŠ” ê°œì¸ë³„ ì˜ë£Œë¹„.

# In[5]:


import pandas as pd
medical_cost_df = pd.read_csv('./datasets/medical_cost.csv')
medical_cost_df


# In[6]:


from sklearn.preprocessing import LabelEncoder

columns = ['sex', 'smoker', 'region']
encoders = []
for column in columns:
    encoder = LabelEncoder()
    category = encoder.fit_transform(medical_cost_df[column])
    medical_cost_df[column] = category
    encoders.append(encoder)


# In[7]:


import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error

def get_evaluation(y_test, prediction):
    MAE =  mean_absolute_error(y_test, prediction)
    MSE = mean_squared_error(y_test, prediction)
    RMSE = np.sqrt(MSE)
    MSLE = mean_squared_log_error(y_test, prediction)
    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))
    R2 = r2_score(y_test, prediction)

    print('MAE: {:.4f}, MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}, R2: {:.4f}'.format(MAE, MSE, RMSE, MSLE, RMSLE, R2))


# In[12]:


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

features, targets = medical_cost_df.iloc[:, :-1], medical_cost_df.charges

X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=124)

y_train = np.log1p(y_train)

linear_regression = LinearRegression()
linear_regression.fit(X_train, y_train)

# ê¸°ìš¸ê¸°(ê°€ì¤‘ì¹˜)
print(linear_regression.coef_)
# ì ˆí¸(ìƒìˆ˜)
print(linear_regression.intercept_)

prediction = linear_regression.predict(X_test)
print(linear_regression.score(X_test, np.log1p(y_test)))
print(r2_score(np.log1p(y_test), prediction))
get_evaluation(np.log1p(y_test), prediction)


# In[11]:


from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

features, targets = medical_cost_df.iloc[:, :-1], medical_cost_df.charges

parmas = {
    'gamma': [0.01, 0.1, 1, 10, 100], 
    'C': [0.01, 0.1, 1, 10, 100], 
    'epsilon': [0, 0.01, 0.1, 1, 10, 100]
}

X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=0)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

grid_svr = GridSearchCV(SVR(kernel='linear'), param_grid=parmas, cv=3, refit=True, return_train_score=True, scoring='r2')


# ë¡œê·¸ ë³€í™˜
y_train = np.log1p(y_train)

grid_svr.fit(X_train, y_train)

prediction = grid_svr.predict(X_test)

# ê¸°ìš¸ê¸°(ê°€ì¤‘ì¹˜)
print(grid_svr.best_estimator_.coef_)

get_evaluation(np.log1p(y_test), prediction)


# In[13]:


# DataFrameìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_svr.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']].sort_values(by='rank_test_score')

