#!/usr/bin/env python
# coding: utf-8

# ### Regularized Linear Regression (ì •ê·œí™”ëœ ì„ í˜• íšŒê·€)
# - ë‹¤ì¤‘ íšŒê·€ ëª¨ë¸ì€ ë³µì¡ë„ê°€ ë†’ì•„ì„œ ê³¼ëŒ€ì í•©(overfitting)ë˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” ê·œì œ(penalty)ë¥¼ ì£¼ì–´ ë³µì¡ë„ë¥¼ ê°ì†Œì‹œì¼œì•¼ í•œë‹¤.
# 
# ##### NORM
# - l<sub>p</sub>-norm ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.
# <img src="./images/regularized01.png" style="margin-left: 0">
# 
# - p=1 ì¼ ë•Œ, l<sub>1</sub>-norm, p=2ì¼ ë•Œ, l<sub>2</sub>-normì´ë‹¤.
# - p=1ì¼ ë•Œì—ëŠ” ë§ˆë¦„ëª¨ í˜•íƒœë¥¼, p=2ì¼ ë•ŒëŠ” ì›ì˜ í˜•íƒœë¥¼ ê°€ì§„ë‹¤.
# <img src="./images/regularized02.png" style="margin-left: 0">
# 
# 
# ##### ë¼ì˜(LASSO, least absolute shrinkage and selection operator)
# - L1 ê·œì œë¥¼ í†µí•œ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤.
# - íŒŒë¼ë¯¸í„°ê°€ 2ê°œì¸ ê²½ìš° íŒŒë€ìƒ‰ ë„í˜•ì€ L1 ê·œì œë¥¼ ë‚˜íƒ€ë‚´ë©°, ì£¼í™©ìƒ‰ ì„ ì€ ì•„ë˜ì˜ ì‹ì„ ì„ í˜•ìœ¼ë¡œ í‘œí˜„í•œ ì í•© íšŒê·€ì„ ì´ë‹¤.
# <div style="display: flex; margin-top:20px">
#     <div>
#         <img src="./images/regression04.png" width="100" style="margin-top:20px; margin-left: 0">
#     </div>
#     <div>
#         <img src="./images/regularized03.png" style="margin-left: 30px">
#     </div>
# </div>
# 
# - ê·œì œí•­ì´ 0ì— ìˆ˜ë ´í• ë•Œ L1 ì •ê·œí™”ì˜ ê²½ìš° ì£¼í™©ìƒ‰ ì„ ì˜ ì ˆí¸ì€ 0ì´ ë  ìˆ˜ ìˆë‹¤.
# - L1 ë…¸ë¦„ì˜ ê²½ìš° ì ˆëŒ“ê°’ì— ëŒ€í•œ ì‹ì´ë¯€ë¡œ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, íŠ¹ì • ë°©ì‹ì„ í†µí•´ ë¯¸ë¶„í•˜ì˜€ì„ ë•Œ ê°€ì¤‘ì¹˜ê°€ 0ì´ë¼ ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—,  
# ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.
# - ì¤‘ìš”í•˜ì§€ ì•Šì€ íŠ¹ì„±ë“¤ì€ ëª¨ë¸ì—ì„œ ì œì™¸í•˜ì—¬ ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ê³ , ê°€ì¥ ì˜í–¥ë ¥ì´ í° íŠ¹ì„±ì´ ë¬´ì—‡ì¸ì§€ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ í•´ì„ë ¥ì´ ì¢‹ì•„ì§„ë‹¤. 
# > - ğŸ“Œì ˆëŒ€ê°’ì— ëŒ€í•œ ì‹ì„ ë¯¸ë¶„í•  ìˆ˜ ì—†ëŠ” ì´ìœ ëŠ”, ê¸°ìš¸ê¸°ê°€ -1ì—ì„œ 1 ì‚¬ì´ì¸ ì§ì„  ëª¨ë‘ê°€ ì ‘ì„ ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì ‘ì ì´ í•œ ê°œ ìˆì„ ê²½ìš° ì„ ì„ ì •í™•íˆ ê·¸ì„ ìˆ˜ ì—†ìœ¼ë©°, ì´ëŠ” ì¢Œê·¹í•œê³¼ ìš°ê·¹í•œì´ ë‹¤ë¥¸ ê²ƒì´ê³  ê·¹í•œì˜ ì •ì˜ì— ì˜í•´ ì–´ë–¤ ê²ƒì— ê°€ê¹Œì›Œì§„ë‹¤ê³  ë‹¨ì •ì§“ê¸° ì• ë§¤í•˜ê¸° ë•Œë¬¸ì—, í•´ë‹¹ ì§€ì ì—ì„œëŠ” ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ë§í•œë‹¤.
# <img src="./images/regularized06.png" width=500 style="margin-left: 0">
# 
# ##### ë¦¿ì§€(Ridge)
# - L2ê·œì œë¥¼ í†µí•œ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤.
# - íŒŒë¼ë¯¸í„°ê°€ 2ê°œì¸ ê²½ìš° íŒŒë€ìƒ‰ ë„í˜•ì€ L2 ê·œì œë¥¼ ë‚˜íƒ€ë‚´ë©°, ì£¼í™©ìƒ‰ ì„ ì€ ì•„ë˜ì˜ ì‹ì„ ì„ í˜•ìœ¼ë¡œ í‘œí˜„í•œ ì í•© íšŒê·€ì„ ì´ë‹¤.
# <div style="display: flex; margin-top:20px">
#     <div>
#         <img src="./images/regression04.png" width="100" style="margin-top:20px; margin-left: 0">
#     </div>
#     <div>
#         <img src="./images/regularized04.png" style="margin-left: 30px">
#     </div>
# </div>
# 
# - ê·œì œí•­ì´ 0ì— ìˆ˜ë ´í• ë•Œ L2 ì •ê·œí™”ì˜ ê²½ìš° ì£¼í™©ìƒ‰ ì„ ì˜ ì ˆí¸ì€ 0ì´ ë  ìˆ˜ ì—†ë‹¤.
# - L2 ë…¸ë¦„ì˜ ê²½ìš° ë¯¸ë¶„ì„ í–ˆì„ ë•Œ ê°€ì¤‘ì¹˜ê°€ ë‚¨ì•„ìˆê¸° ë•Œë¬¸ì—, ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì—ëŠ” ì í•©í•˜ë‹¤.
# - ê°’ì´ 0ì´ ë˜ëŠ”(ì œì™¸í•˜ëŠ”) íŠ¹ì„±ì€ ì—†ì§€ë§Œ, ê³¨ê³ ë£¨ 0ì— ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ì¼ë¶€ë¡œ ëœ í•™ìŠµì‹œì¼œì„œ ì¥ê¸°ì ìœ¼ë¡œ ë” ì¢‹ì€ ëª¨ë¸ì´ ëœë‹¤.
# 
# ##### Î» (Regulation parameter)
# - Î»ì´ ì»¤ì§€ë©´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •ì—ì„œ ë…¸ë¦„(norm)ì´ ì‘ì•„ì§€ë¯€ë¡œ ê·œì œê°€ ê°•í•´ì¡Œë‹¤ê³  í‘œí˜„í•œë‹¤.
# - ë…¸ë¦„(norm)ì´ ì»¤ì§€ë©´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •ì—ì„œ Î»ì´ ì‘ì•„ì§€ë¯€ë¡œ ê·œì œê°€ ì•½í•´ì¡Œë‹¤ê³  í‘œí˜„í•œë‹¤.
# <img src="./images/regularized05.png" width="350" style="margin-left: 0">
# 
# ##### ğŸš©ê²°ë¡ : ì—¬ëŸ¬ feature ì¤‘ ì¼ë¶€ë¶„ë§Œ ì¤‘ìš”í•˜ë©´ ë¼ì˜, ì¤‘ìš”ë„ê°€ ì „ì²´ì ìœ¼ë¡œ ë¹„ìŠ·í•˜ë©´ ë¦¿ì§€ë¥¼ ì‚¬ìš©í•˜ì!

# ### Polynomial Regression Task
# 
# ##### í•œêµ­ì¸ ìˆ˜ìµ ì˜ˆì¸¡
# - id : ì‹ë³„ ë²ˆí˜¸
# - year : ì¡°ì‚¬ ë…„ë„
# - wave : 2005ë…„ wave 1ìœ„ë¶€í„° 2018ë…„ wave 14ìœ„ê¹Œì§€
# - region: 1)ì„œìš¸ 2)ê²½ê¸° 3)ê²½ë‚¨ 4)ê²½ë¶ 5)ì¶©ë‚¨ 6)ê°•ì› & ì¶©ë¶ 7)ì „ë¼ & ì œì£¼
# - income: ì—°ê°„ ìˆ˜ì… Mì›(ë°±ë§Œì›.1100ì›=1USD)
# - family_member: ê°€ì¡± êµ¬ì„±ì› ìˆ˜
# - gender: 1) ë‚¨ì„± 2) ì—¬ì„±
# - year_born: íƒœì–´ë‚œ ë…„ë„
# - education_level:1)ë¬´êµìœ¡(7ì„¸ ë¯¸ë§Œ) 2)ë¬´êµìœ¡(7ì„¸ ì´ìƒ) 3)ì´ˆë“±í•™êµ 4)ì¤‘í•™êµ 5)ê³ ë“±í•™êµ 6)ëŒ€í•™ í•™ìœ„ 8)MA 9)ë°•ì‚¬ í•™ìœ„
# - marriage: í˜¼ì¸ìƒíƒœ. 1)í•´ë‹¹ì—†ìŒ(18ì„¸ ë¯¸ë§Œ) 2)í˜¼ì¸ì¤‘ 3)ì‚¬ë§ìœ¼ë¡œ ë³„ê±°ì¤‘ 4)ë³„ê±°ì¤‘ 5)ë¯¸í˜¼ 6)ê¸°íƒ€
# - religion: 1) ì¢…êµ ìˆìŒ 2) ì¢…êµ ì—†ìŒ  
# - occupation: ì§ì¢… ì½”ë“œ, ë³„ë„ ì²¨ë¶€
# - company_size: ê¸°ì—… ê·œëª¨
# - reason_none_worker: 1)ëŠ¥ë ¥ ì—†ìŒ 2)êµ° ë³µë¬´ ì¤‘ 3)í•™êµì—ì„œ ê³µë¶€ ì¤‘ 4)í•™êµ ì¤€ë¹„ 5)ì§ì¥ì¸ 7)ì§‘ì—ì„œ ëŒë³´ëŠ” ì•„ì´ë“¤ 8)ê°„í˜¸ 9)ê²½ì œ í™œë™ í¬ê¸° 10)ì¼í•  ì˜ì‚¬ ì—†ìŒ 11)ê¸°íƒ€

# In[2]:


import pandas as pd

income_df = pd.read_csv('./datasets/korean_income.csv')
income_df.info()


# In[3]:


income_df = income_df[income_df.income > 0.0]
income_df.shape[0]


# In[4]:


income_df.loc[:, 'occupation'] = income_df.occupation.apply(lambda x: x.replace(' ', '0'))
income_df.loc[:, 'company_size'] = income_df.company_size.apply(lambda x: x.replace('99', '0'))
income_df.loc[:, 'company_size'] = income_df.company_size.apply(lambda x: x.replace(' ', '0'))
income_df.loc[:, 'reason_none_worker'] = income_df.reason_none_worker.apply(lambda x: x.replace('99', '12'))
income_df.loc[:, 'reason_none_worker'] = income_df.reason_none_worker.apply(lambda x: x.replace(' ', '12'))


# In[5]:


income_df['target'] = income_df.income
income_df = income_df.drop(columns='income', axis=1)


# In[6]:


income_df[['company_size', 'occupation', 'reason_none_worker']] = income_df[['company_size', 'occupation', 'reason_none_worker']].astype('int16')
income_df.info()


# In[7]:


income_df = income_df.drop(columns='id', axis=1)


# In[8]:


income_df = income_df.reset_index(drop=True)
income_df


# In[9]:


from sklearn.preprocessing import StandardScaler
income_df = income_df[pd.Series(StandardScaler().fit_transform(income_df[['target']]).flatten()).between(-1.96, 1.96)]
income_df


# In[14]:


import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error

def get_evaluation(y_test, prediction):
    MAE =  mean_absolute_error(y_test, prediction)
    MSE = mean_squared_error(y_test, prediction)
    RMSE = np.sqrt(MSE)
    MSLE = mean_squared_log_error(y_test, prediction)
    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))
    R2 = r2_score(y_test, prediction)

    print('MAE: {:.4f}, MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}, R2: {:.4f}'.format(MAE, MSE, RMSE, MSLE, RMSLE, R2))


# In[16]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

features, targets = income_df.iloc[:, :-1], income_df.target

poly_features = PolynomialFeatures(degree=3).fit_transform(features)

X_train, X_test, y_train, y_test = train_test_split(poly_features, targets, test_size=0.2, random_state=0)

# ë¡œê·¸ ë³€í™˜
y_train = np.log1p(y_train)

linear_regression = LinearRegression()

linear_regression.fit(X_train, y_train)

prediction = linear_regression.predict(X_test)

get_evaluation(np.log1p(y_test), prediction)


# In[21]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures

features, targets = income_df.iloc[:, :-1], income_df.target

poly_features = PolynomialFeatures(degree=3).fit_transform(features)

X_train, X_test, y_train, y_test = train_test_split(poly_features, targets, test_size=0.2, random_state=0)

# ë¡œê·¸ ë³€í™˜
y_train = np.log1p(y_train)

lasso = Lasso(max_iter=500)

lasso.fit(X_train, y_train)

prediction = lasso.predict(X_test)
print(lasso.coef_)
# prediction[prediction < 0] = np.log1p(2.0)

# MAE: 0.3556, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

# alpha = 0.001
# MAE: 0.3709, MSE: 0.2420, RMSE: 0.4920, MSLE: 0.0037, RMSLE: 0.0610, R2: 0.6865

# alpha = 0.01
# MAE: 0.3712, MSE: 0.2422, RMSE: 0.4921, MSLE: 0.0037, RMSLE: 0.0610, R2: 0.6863

# alpha = 1
# MAE: 0.3738, MSE: 0.2447, RMSE: 0.4947, MSLE: 0.0038, RMSLE: 0.0613, R2: 0.6830

# alpha = 100
# MAE: 0.3794, MSE: 0.2505, RMSE: 0.5005, MSLE: 0.0038, RMSLE: 0.0618, R2: 0.6754

# alpha = 1000
# MAE: 0.3917, MSE: 0.2647, RMSE: 0.5145, MSLE: 0.0040, RMSLE: 0.0633, R2: 0.6571

get_evaluation(np.log1p(y_test), prediction)


# In[27]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures

features, targets = income_df.iloc[:, :-1], income_df.target

poly_features = PolynomialFeatures(degree=3).fit_transform(features)

X_train, X_test, y_train, y_test = train_test_split(poly_features, targets, test_size=0.2, random_state=0)

# ë¡œê·¸ ë³€í™˜
y_train = np.log1p(y_train)

ridge = Ridge(max_iter=500, alpha = 1000)

ridge.fit(X_train, y_train)

prediction = ridge.predict(X_test)
print(ridge.coef_)
# prediction[prediction < 0] = np.log1p(2.0)

# MAE: 0.3556, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

# alpha = 0.001
# MAE: 0.3557, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

# alpha = 0.01
# MAE: 0.3557, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

# alpha = 1
# MAE: 0.3557, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

# alpha = 100
# MAE: 0.3557, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

# alpha = 1000
# MAE: 0.3557, MSE: 0.2283, RMSE: 0.4778, MSLE: 0.0035, RMSLE: 0.0596, R2: 0.7043

get_evaluation(np.log1p(y_test), prediction)

