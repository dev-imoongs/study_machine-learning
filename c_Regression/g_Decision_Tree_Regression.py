#!/usr/bin/env python
# coding: utf-8

# ### Decision Tree Regression (íšŒê·€ íŠ¸ë¦¬)
# - ê²°ì • íŠ¸ë¦¬ì™€ ê²°ì • íŠ¸ë¦¬ ê¸°ë°˜ì˜ ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜ì€ ë¶„ë¥˜ë¿ë§Œ ì•„ë‹ˆë¼ íšŒê·€ë„ ê°€ëŠ¥í•˜ë‹¤.
# - ë¶„ë¥˜ì™€ ìœ ì‚¬í•˜ê²Œ ë¶„í• ì„ í•˜ë©°, ìµœì¢… ë¶„í•  í›„ ê° ë¶„í•  ì˜ì—­ì—ì„œ ì‹¤ì œ ë°ì´í„°ê¹Œì§€ì˜ ê±°ë¦¬ë“¤ì˜ í‰ê·  ê°’ìœ¼ë¡œ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.
# <img src="./images/decision_tree_regression01.png" width="600" style="margin: 10px; margin-left: 0">
# - íšŒê·€ íŠ¸ë¦¬ ì—­ì‹œ ë³µì¡í•œ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ê²½ìš° ê³¼ì í•©ì´ ìœ„í—˜ì´ ìˆê³ , íŠ¸ë¦¬ì˜ í¬ê¸°ì™€ ë…¸ë“œì˜ ê°œìˆ˜ì˜ ì œí•œ ë“±ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆë‹¤.
# <img src="./images/decision_tree_regression02.png" width="600" style="margin:20px; margin-left: 0">
# - ë…ë¦½ ë³€ìˆ˜ë“¤ê³¼ ì¢…ì† ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ê°€ ìƒë‹¹íˆ ë¹„ì„ í˜•ì ì¼ ê²½ìš° ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
# <img src="./images/decision_tree_regression03.png" width="800" style="margin:20px; margin-left: 0">
# - ğŸš© í•˜ì§€ë§Œ, ë‹¤ë¥¸ íšŒê·€ ëª¨ë¸ë³´ë‹¤ ì „ì²´ì ì¸ ì„±ëŠ¥ì€ ë–¨ì–´ì§„ë‹¤.

# ##### í•œìš° ê°€ê²© ì˜ˆì¸¡

# In[138]:


import chardet

rawdata = open('./datasets/korea_cow.csv', 'rb').read()
result = chardet.detect(rawdata)
charenc = result['encoding']
charenc


# In[139]:


import pandas as pd

cow_df = pd.read_csv('./datasets/korea_cow.csv', encoding='euc-kr')
cow_df


# In[140]:


cow_df.isna().sum()


# In[141]:


cow_df = cow_df.drop(columns=['ê°œì²´ë²ˆí˜¸', 'ì¶œí•˜ì£¼', 'kpn', 'ë¹„ê³ ', 'ìµœì €ê°€', 'ì¼ì', 'ë²ˆí˜¸', 'ì§€ì—­'])
cow_df.isna().sum()


# In[142]:


cow_df.info()


# In[143]:


cow_df.ì¢…ë¥˜.value_counts()


# In[144]:


cow_df.ìƒíƒœ.value_counts()


# In[145]:


cow_df = cow_df[cow_df.ìƒíƒœ == 'ë‚™ì°°']
cow_df


# In[146]:


cow_df = cow_df.drop(columns='ìƒíƒœ', axis=1)


# In[147]:


cow_df


# In[148]:


cow_df.describe().T


# In[149]:


from matplotlib import font_manager

plt.rc('font', family='Malgun Gothic')
cow_df.hist()


# In[150]:


from matplotlib import font_manager
import numpy as np

plt.rc('font', family='Malgun Gothic')
np.log1p(cow_df.ë‚™ì°°ê°€).hist()


# In[151]:


cow_df['ê°€ê²©'] = cow_df.ë‚™ì°°ê°€


# In[152]:


cow_df = cow_df.drop(columns='ë‚™ì°°ê°€', axis=1)


# In[153]:


cow_df


# In[154]:


cow_df.ì„±ë³„.value_counts()


# In[155]:


cond1 = cow_df.ì„±ë³„ == 'ìˆ˜' 
cond2 = cow_df.ì„±ë³„ == 'ì•”'
cond = cond1 | cond2
cow_df = cow_df[cond]
cow_df.ì„±ë³„.value_counts()


# In[156]:


cow_df.info()


# In[157]:


from sklearn.preprocessing import LabelEncoder

encoders = []
for column in cow_df[['ì„±ë³„', 'ì¢…ë¥˜']]:
    encoder = LabelEncoder()
    cow_df.loc[:, column] = encoder.fit_transform(cow_df[column])
    encoders.append(encoder)
    print(encoder.classes_)


# In[158]:


from sklearn.preprocessing import StandardScaler

scale = StandardScaler()
scaled_cow_df = pd.DataFrame(scale.fit_transform(cow_df.iloc[:, :-1]), columns=cow_df.iloc[:, :-1].columns)


# In[159]:


scaled_cow_df


# In[160]:


from matplotlib import font_manager

plt.rc('font', family='Malgun Gothic')
scaled_cow_df.hist()


# In[161]:


scaled_cow_df = scaled_cow_df[scaled_cow_df.ì¤‘ëŸ‰.between(-1.96, 1.96)]
scaled_cow_df = scaled_cow_df[scaled_cow_df.ê³„ëŒ€.between(-1.96, 1.96)]


# In[162]:


from matplotlib import font_manager

plt.rc('font', family='Malgun Gothic')
scaled_cow_df.hist()


# In[165]:


cow_df = cow_df.iloc[scaled_cow_df.index, :]
cow_df


# In[166]:


cow_df = cow_df.reset_index(drop=True)
cow_df


# In[167]:


scaled_cow_df = scaled_cow_df.reset_index(drop=True)
scaled_cow_df['ê°€ê²©'] = cow_df.ê°€ê²©
scaled_cow_df


# In[168]:


import seaborn as sns
import matplotlib.pyplot as plt

corr = scaled_cow_df.corr()
fig = plt.figure(figsize=(7, 5))
heatmap = sns.heatmap(corr, annot=True)
heatmap.set_title("Correlation")


# In[169]:


import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error

def get_evaluation(y_test, prediction):
    MAE =  mean_absolute_error(y_test, prediction)
    MSE = mean_squared_error(y_test, prediction)
    RMSE = np.sqrt(MSE)
    MSLE = mean_squared_log_error(y_test, prediction)
    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))
    R2 = r2_score(y_test, prediction)

    print('MAE: {:.4f}, MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}, R2: {:.4f}'.format(MAE, MSE, RMSE, MSLE, RMSLE, R2))


# In[170]:


scaled_cow_df.ê°€ê²©.isna().sum()


# In[171]:


cow_df = cow_df[~cow_df.ê³„ëŒ€.isna()]
cow_df


# In[173]:


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

features, targets = cow_df.drop(columns='ê°€ê²©', axis=1), cow_df.ê°€ê²©

poly_features = PolynomialFeatures(degree=3).fit_transform(features)

X_train, X_test, y_train, y_test = train_test_split(poly_features, targets, test_size=0.3, random_state=124)


# In[175]:


from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import numpy as np

dt_reg = DecisionTreeRegressor(random_state=124, max_depth=4)
rf_reg = RandomForestRegressor(random_state=124, n_estimators=1000, max_depth=8)
gb_reg = GradientBoostingRegressor(random_state=124, n_estimators=1000, max_depth=8)
xgb_reg = XGBRegressor(n_estimators=1000, max_depth=8)
lgb_reg = LGBMRegressor(n_estimators=1000, max_depth=8)

# íŠ¸ë¦¬ ê¸°ë°˜ì˜ íšŒê·€ ëª¨ë¸ì„ ë°˜ë³µí•˜ë©´ì„œ í‰ê°€ ìˆ˜í–‰ 
models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]
for model in models:  
    model.fit(X_train, np.log1p(y_train))
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation(np.log1p(y_test), prediction)

