#!/usr/bin/env python
# coding: utf-8

# ### Dimension Reduction (ì°¨ì› ì¶•ì†Œ)
# - ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°ë“¤ì€ ë³´í†µ 3ì°¨ì› ê³µê°„ì—ì„  í‘œí˜„í•˜ê¸° í˜ë“  ê³ ì°¨ì›(high dimension)ì˜ ë°ì´í„°ì¸ ê²½ìš°ê°€ ë§ë‹¤.
# - ì°¨ì›ì´ ì»¤ì§ˆ ìˆ˜ë¡ ë°ì´í„° ê°„ ê±°ë¦¬ê°€ í¬ê²Œ ëŠ˜ì–´ë‚˜ë©°, ë°ì´í„°ê°€ í¬ì†Œí™”(Spares) ëœë‹¤(ì°¨ì›ì˜ ì €ì£¼).
# > - í¬ì†Œ ë°ì´í„°(Spares Data): ì°¨ì›/ì „ì²´ ê³µê°„ì— ë¹„í•´ ë°ì´í„°ê°€ ìˆëŠ” ê³µê°„ì´ ë§¤ìš° í˜‘ì†Œí•œ ë°ì´í„°
# > - ë°€ì§‘ ë°ì´í„°(Dense Data): ì°¨ì›/ì „ì²´ ê³µê°„ì— ë¹„í•´ ë°ì´í„°ê°€ ìˆëŠ” ê³µê°„ì´ ë¹½ë¹½í•˜ê²Œ ì°¨ ìˆëŠ” ë°ì´í„°
# - ê³ ì°¨ì›ì„ ì´ë£¨ëŠ” í”¼ì²˜ ì¤‘ ìƒëŒ€ì ìœ¼ë¡œ ì¤‘ìš”ë„ê°€ ë–¨ì–´ì§€ëŠ” í”¼ì²˜ê°€ ì¡´ì¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê³„ì‚° ë¹„ìš©ì´ ë§ê³  ë¶„ì„ì— í•„ìš”í•œ ì‹œê°í™”ê°€ ì–´ë µë‹¤.
# - ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë©°, í¬ì†Œ ë°ì´í„°ë¥¼ í•™ìŠµ ì‹œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤.
# - ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ Spares Dataë¥¼ Denseí•˜ê²Œ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤.
# - featureê°€ ë§ì„ ê²½ìš° feautureê°„ ìƒê´€ê´€ê³„ê°€ ë†’ì•„ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ê³ , ì´ë¡œ ì¸í•´ ì„ í˜• íšŒê·€ ëª¨ë¸ì—ì„œ ë‹¤ì¤‘ ê³µì„ ì„±(Multicollinearity) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
# - ì°¨ì› ì¶•ì†Œë¡œ ì¸í•´ í‘œí˜„ë ¥ì´ ì¼ë¶€ ì†ì‹¤ë˜ì§€ë§Œ, ì†ì‹¤ì„ ê°ìˆ˜í•˜ë”ë¼ë„ ê³„ì‚° íš¨ìœ¨ì„ ì–»ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.
# ---
# 
# #### PCA (Principal Component Analysis), ì£¼ì„±ë¶„ ë¶„ì„
# - ê³ ì°¨ì›(x<sub>1</sub>, Â·Â·Â·, x<sub>n</sub>)ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì› (x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>)ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ëŒ€í‘œì ì¸ ì°¨ì› ì¶•ì†Œ(Dimension reduction) ë°©ë²•ì´ë‹¤.
# - ë°ì´í„°ì˜ íŠ¹ì„±ì„ ëˆˆìœ¼ë¡œ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, ì—°ì‚° ì†ë„ì— í° ì´ì ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
# - ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ë°ì´í„°ë¡œ ì••ì¶•í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì¶•(Principal Component(PC), ì£¼ì„±ë¶„)ì„ ì„¤ì •í•´ì•¼ í•œë‹¤.
# - ì£¼ì„±ë¶„ ë¶„ì„ì„ í†µí•´ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•  ë•Œ feature ì¶”ì¶œ(feature extraction)ì„ ì§„í–‰í•œë‹¤. feature ì¶”ì¶œì€ ê¸°ì¡´ í”¼ì²˜ë¥¼ ì¡°í•©í•´ ìƒˆë¡œìš´ featureë¡œ ë§Œë“œëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ìƒˆë¡œìš´ featureì™€ ê¸°ì¡´ feature ê°„ ì—°ê´€ì„±ì´ ì—†ì–´ì•¼ í•œë‹¤.
# - ì—°ê´€ì„±ì´ ì—†ë„ë¡ í•˜ê¸° ìœ„í•´ì„œëŠ” ë‚´ì í–ˆì„ ë•Œ 0ì´ ë‚˜ì™€ì•¼ í•˜ê³  ì´ëŠ” ì„œë¡œ ì§êµ(ì§ê°, 90ë„)í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ì§êµ ë³€í™˜ì„ ìˆ˜í–‰í•œë‹¤.
# - ì§êµ ë³€í™˜ì„ ìˆ˜í–‰í•  ë•Œ ê¸°ì¡´ featureì™€ ê´€ë ¨ ì—†ìœ¼ë©´ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ í‘œí˜„ë ¥ì„ ì˜ ë³´ì¡´í•´ì•¼í•˜ê³  ì´ëŠ” ë‹¤ë¥¸ ë§ë¡œ "ë¶„ì‚°(Variance)ì„ ìµœëŒ€ë¡œ í•˜ëŠ” ì£¼ì¶•ì„ ì°¾ëŠ”ë‹¤" ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
# - ì˜ˆë¥¼ ë“¤ì–´, 2ì°¨ì› ê³µê°„ì´ ìˆê³  1ì°¨ì› ê³µê°„ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œ í•œë‹¤ë©´ 1ì°¨ì› ê³µê°„ìƒì—ì„œ ë°ì´í„° ë¶„í¬ê°€ ê°€ì¥ ë„“ê²Œ í¼ì§€ê²Œ ë§Œë“œëŠ” ë²¡í„°(Eigen vector, ê³ ìœ  ë²¡í„°)ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.
# - ì°¾ì€ ê³ ìœ  ë²¡í„° ì¶•ì— featureë“¤ì„ íˆ¬ì˜ì‹œí‚¤ë©´ ì´ê²Œ ë°”ë¡œ ì£¼ì„±ë¶„ì´ ëœë‹¤.
# - Eigen vectorë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ê³ ìœ ê°’ ë¶„í•´(EVD, Eigen Value decomposition) í˜¹ì€ íŠ¹ì´ê°’ ë¶„í•´ (SVD, Singular Value Decomposition)ê°€ ìˆ˜í–‰ë˜ì–´ì•¼ í•˜ê³ , ì´ë¥¼ í†µí•´ êµ¬í•œ ì£¼ì¶•ì— íˆ¬ì˜ì„ í•˜ê¸° ìœ„í•´ì„œ ê³µë¶„ì‚° í–‰ë ¬ì´ í•„ìš”í•˜ë‹¤. ì¦‰, PCAëŠ” ë°ì´í„°ë“¤ì˜ ê³µë¶„ì‚° í–‰ë ¬ì— ëŒ€í•œ íŠ¹ì´ê°’ ë¶„í•´(SVD)ì´ë‹¤.
# 
# <div style="display: flex">
#     <div>
#         <img src="./images/pca02.gif" style="margin-left: -200px">
#     </div>
#     <div>
#         <img src="./images/pca01.gif" width="700" style="margin-top:50px; margin-left: -350px">
#     </div>
# </div>
# 
# > ##### ê³µë¶„ì‚° í–‰ë ¬ (Covariance matrix)
# > - ê³µë¶„ì‚°ì´ë€, ë‘ í”¼ì²˜ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•˜ë©° êµ¬í•˜ëŠ” ê³µì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.
# <img src="./images/pca03.png" width="150" style="margin:20px; margin-left: 0">
# > - nì€ í–‰ë ¬ Xì— ìˆëŠ” ë°ì´í„° ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©° Xì˜ ì—´ì¶•ì€ featureì´ê³  Xì˜ í–‰ì¶•ì€ ë°ì´í„° ê°œìˆ˜ì´ë‹¤.
# > - ì˜ˆë¥¼ ë“¤ì–´, Xì˜ featureê°€ x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>ë¼ê³  ê°€ì •í•˜ë©´, X<sup>T</sup> * XëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
# <img src="./images/pca04.png" width="250" style="margin:20px; margin-left: 0">
# > - ì „ì¹˜ í–‰ë ¬ê³¼ í–‰ë ¬ì„ ë‚´ì í•˜ë©´, ì•„ë˜ì™€ ê°™ì€ ëŒ€ì¹­í–‰ë ¬ì´ ë§Œë“¤ì–´ì§€ë©°, ëª¨ë“  ëŒ€ê° ì„±ë¶„ì€ ê°™ì€ í”¼ì²˜ ê°„ ë‚´ì ì´ë¯€ë¡œ ë¶„ì‚°ì— í•´ë‹¹í•œë‹¤.
# <img src="./images/pca05.png" width="250" style="margin:20px; margin-left: 0">
# > - feature ê°„ ë‚´ì ì„ í†µí•´ ìˆ˜ì¹˜ì ìœ¼ë¡œ ië²ˆ í”¼ì²˜ì™€ jë²ˆ í”¼ì²˜ê°€ ê³µë³€í•˜ëŠ” ì •ë„ë¥¼ ì•Œ ìˆ˜ ìˆê²Œ ë˜ê³ , ê³µë¶„ì‚°ì´ ì»¤ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°ì´í„° ê°œìˆ˜(n)ë¡œ ë‚˜ëˆ„ì–´ í‰ê· ì„ êµ¬í•œë‹¤.<br>
# > ğŸš©ìœ„ì™€ ê°™ì€ í–‰ë ¬ì„ ê³µë¶„ì‚° í–‰ë ¬ì´ë¼ í•˜ê³ , ì„ì˜ì˜ ë²¡í„°ê°€ ìˆì„ ë•Œ ê³µë¶„ì‚° í–‰ë ¬ì„ ê³±í•´ì£¼ê²Œ ë˜ë©´ ê·¸ ë²¡í„°ì˜ ì„ í˜• ë³€í™˜(íˆ¬ì˜)ì´ ì´ë£¨ì–´ì§„ë‹¤.
# 
# > ##### ê³ ìœ ê°’ ë¶„í•´(Eigen Value decomposition)ì™€ íŠ¹ì´ê°’ ë¶„í•´ (Singular Value Decomposition)
# > - ê³ ìœ ê°’ ë¶„í•´ê°€ ì£¼ì¶•ì„ ì´ë£¨ëŠ” ë²¡í„°ë¥¼ ì°¾ëŠ” ê²ƒì´ë¼ë©´, íŠ¹ì´ê°’ ë¶„í•´ëŠ” ì§êµí•˜ëŠ” ë²¡í„°ë“¤ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.
# > - ê³ ìœ ê°’ ë¶„í•´ëŠ” íˆ¬ì˜ì‹œ í¬ê¸°ë§Œ ë³€í•˜ê³  ì—¬ì „íˆ ë°©í–¥ì´ ë³€í•˜ì§€ ì•ŠëŠ” ë²¡í„°(V)ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë©°, íŠ¹ì´ê°’ ë¶„í•´ëŠ” ì§êµí•˜ëŠ” ë²¡í„°ë¥¼ íˆ¬ì˜ ì‹œ ì—¬ì „íˆ ì§êµí•˜ëŠ” ë²¡í„°ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.
# > - ê³µë¶„ì‚° í–‰ë ¬ì„ í†µí•´ ê³ ìœ ê°’ ë˜ëŠ” íŠ¹ì´ê°’ì„ ë¶„í•´í•¨ìœ¼ë¡œì¨ PCAì— í•„ìš”í•œ ì£¼ì¶•ì¸ ê³ ìœ  ë²¡í„°ì™€, ê³ ìœ  ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
# > - ê³ ìœ ê°’ì„ ì–»ì€ ë’¤ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í–ˆì„ ë•Œ ê°€ì¥ ì²« ë²ˆì§¸ ê°’ì´ ë¶„ì‚°ì„ ìµœëŒ€ë¡œ í•˜ëŠ” ê°’ì´ ëœë‹¤.
# > - ê³ ìœ ê°’ ë¶„í•´ëŠ” í–‰ë ¬ì˜ í¬ê¸°ê°€ ì»¤ì§ˆ ìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ ì¦ê°€í•˜ë¯€ë¡œ ê³„ì‚° ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë©°, ëŒ€ì¹­ í–‰ë ¬(m * m)ì´ ì•„ë‹ˆë¼ë©´ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.
# > - sklearnì˜ PCAëŠ” ë‚´ë¶€ì ìœ¼ë¡œ SVDë¥¼ ì‚¬ìš©í•œë‹¤.
# 
# #### LDA (Linear Discriminant Analysis), ì„ í˜• íŒë³„ ë¶„ì„
# - PCAì™€ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, ë¶„ë¥˜ì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ë„ë¡ ê°œë³„ í´ë˜ìŠ¤ë¥¼ ë¶„ë³„í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤.
# - PCAëŠ” ê°€ì¥ í° ë¶„ì‚°ì„ ê°€ì§€ëŠ” ì¶•ì„ ì°¾ì•˜ì§€ë§Œ, LDAëŠ” ì…ë ¥ ë°ì´í„°ì˜ í´ë˜ìŠ¤(ì¹´í…Œê³ ë¦¬)ë¥¼ ìµœëŒ€í•œ ë¶„ë¦¬í•  ìˆ˜ ìˆëŠ” ì¶•ì„ ì°¾ëŠ”ë‹¤.  
# ì¦‰, ê°™ì€ í´ë˜ìŠ¤(ì¹´í…Œê³ ë¦¬)ì˜ ë°ì´í„°ì— ìµœëŒ€í•œ ê·¼ì ‘í•´ì„œ, ë‹¤ë¥¸ í´ë˜ìŠ¤(ì¹´í…Œê³ ë¦¬)ì˜ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ë–¨ì–´ëœ¨ë¦¬ëŠ” ì¶•ì„ ë§¤í•‘í•œë‹¤.
# <div style="display: flex">
#     <div>
#         <img src="./images/lda01.png" width="650" style="margin:20px; margin-left: -20px">
#     </div>
#     <div>
#         <img src="./images/lda02.png" width="650" style="margin:20px; margin-left: 0">
#     </div>
# </div>
# 
# - í´ë˜ìŠ¤ë¥¼ ìµœëŒ€í•œ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ì„œ í´ë˜ìŠ¤ ê°„ ë¶„ì‚° ì„ ìµœëŒ€í™” í•˜ê³  í´ë˜ìŠ¤ ë‚´ë¶€ ë¶„ì‚°ì„ ìµœì†Œí™” í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤.
# - í´ë˜ìŠ¤ë³„ ì‚°í¬ í–‰ë ¬ì„ êµ¬í•˜ì—¬ ì´ë¥¼ í†µí•´ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ê³¼ í´ë˜ìŠ¤ ë‚´ë¶€ ë¶„ì‚°ì„ êµ¬í•œë‹¤.
# <img src="./images/lda03.png" width="250" style="margin:20px; margin-left: 0">

# ##### PCA

# ##### íšŒì‚¬ íŒŒì‚° ë°ì´í„°

# In[1]:


import pandas as pd

company_df = pd.read_csv('./datasets/company.csv')
print(company_df.shape)
company_df.columns


# In[2]:


company_df


# In[3]:


company_df.isna().sum().sum()


# In[4]:


company_df.duplicated().sum()


# In[5]:


company_df['Bankrupt?'].value_counts()


# In[6]:


from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=124)
featrues, targets = company_df.iloc[:, 1:], company_df['Bankrupt?']
over_features, over_targets = smote.fit_resample(featrues, targets)

print('SMOTE ì ìš© ì „:\n',pd.Series(targets).value_counts() )
print('SMOTE ì ìš© í›„:\n',pd.Series(over_targets).value_counts() )


# In[7]:


over_company_df = pd.DataFrame(over_features, columns=company_df.iloc[:, 1:].columns)
over_company_df.shape


# In[8]:


over_company_df


# In[9]:


from sklearn.preprocessing import StandardScaler

over_company_scaled = StandardScaler().fit_transform(over_company_df)
over_company_scaled.shape


# In[10]:


over_company_scaled_df = pd.DataFrame(over_company_scaled, columns=company_df.iloc[:, 1:].columns)
over_company_scaled_df['target'] = over_targets


# In[11]:


over_company_scaled_df


# In[13]:


for column in company_df.iloc[:, 1:].columns:
    over_company_scaled_df = over_company_scaled_df[over_company_scaled_df[column].between(-1.96, 1.96)]

over_company_scaled_df.shape


# In[14]:


over_company_scaled_df.target.value_counts()


# In[18]:


over_company_df = pd.DataFrame(over_features, columns=company_df.iloc[:, 1:].columns)
over_company_df['target'] = over_targets
over_company_df.shape


# In[19]:


over_company_scaled_df


# In[20]:


over_company_df = over_company_df.iloc[over_company_scaled_df.index, :]
over_company_df.shape


# In[21]:


over_company_df = over_company_df.reset_index(drop=True)
over_company_scaled_df = over_company_scaled_df.reset_index(drop=True)
print(over_company_df.shape[0], over_company_scaled_df.shape[0])


# In[22]:


from sklearn.decomposition import PCA

pca = PCA(n_components=2)

company_pca = pca.fit_transform(over_company_scaled_df.iloc[:, :-1])
print(company_pca.shape)


# In[23]:


# PCA í™˜ëœ ë°ì´í„°ì˜ ì»¬ëŸ¼ëª…ì„ ê°ê° pca1, pca2, ..., pcanìœ¼ë¡œ ëª…ëª…
pca_columns=[f'pca{i+1}' for i in range(2)]
company_pca_df = pd.DataFrame(company_pca, columns=pca_columns)
company_pca_df['target']=over_company_scaled_df.target
company_pca_df.head(10)


# In[24]:


print(pca.explained_variance_ratio_)
print(pca.explained_variance_ratio_.sum())


# In[25]:


import seaborn as sns

sns.scatterplot(x="pca1", y="pca2", hue='target', data=company_pca_df, alpha=0.5)


# In[27]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

rfc = RandomForestClassifier(random_state=156, max_depth=3, min_samples_split=30)
scores = cross_val_score(rfc, over_company_df.iloc[:, 1:], over_company_df.target, scoring='accuracy', cv=5)
print('ì›ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê°œë³„ ì •í™•ë„:',scores)
print('ì›ë³¸ ë°ì´í„° í‰ê·  ì •í™•ë„:', np.mean(scores))


# In[28]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

rfc = RandomForestClassifier(random_state=156, max_depth=3, min_samples_split=30)
scores = cross_val_score(rfc, company_pca_df.iloc[:, :-1], company_pca_df.target, scoring='accuracy', cv=5)
print('PCA ë°ì´í„° êµì°¨ ê²€ì¦ ê°œë³„ ì •í™•ë„:',scores)
print('PCA ë°ì´í„° í‰ê·  ì •í™•ë„:', np.mean(scores))


# ##### LDA

# In[30]:


from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# íƒ€ê²Ÿ í´ë˜ìŠ¤(ì¹´í…Œê³ ë¦¬) ê°œìˆ˜ -nì„ ì „ë‹¬í•œë‹¤.
lda = LinearDiscriminantAnalysis(n_components=1)

lda.fit(over_company_scaled_df.iloc[:, :-1], over_company_scaled_df.target)
company_lda = lda.transform(over_company_scaled_df.iloc[:, :-1])

print(company_lda.shape)


# In[31]:


company_lda_df = pd.DataFrame()
company_lda_df['lda']=company_lda.flatten()
company_lda_df['target']=over_company_scaled_df.target
company_lda_df.head(10)


# In[32]:


print(lda.explained_variance_ratio_)
print(lda.explained_variance_ratio_.sum())


# In[33]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

rfc = RandomForestClassifier(random_state=156, max_depth=3, min_samples_split=30)
scores = cross_val_score(rfc, over_company_df.iloc[:, 1:], over_company_df.target, scoring='accuracy', cv=5)
print('ì›ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê°œë³„ ì •í™•ë„:',scores)
print('ì›ë³¸ ë°ì´í„° í‰ê·  ì •í™•ë„:', np.mean(scores))


# In[34]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

rfc = RandomForestClassifier(random_state=156, max_depth=3, min_samples_split=30)
scores = cross_val_score(rfc, company_lda_df[['lda']], company_lda_df.target, scoring='accuracy', cv=5)
print('LDA ë°ì´í„° êµì°¨ ê²€ì¦ ê°œë³„ ì •í™•ë„:',scores)
print('LDA ë°ì´í„° í‰ê·  ì •í™•ë„:', np.mean(scores))

