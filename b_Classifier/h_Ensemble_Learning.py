#!/usr/bin/env python
# coding: utf-8

# ### ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)
# ##### ê·¸ë¦¼ ì¶œì²˜: grokking-machine-learning(ë£¨ì´ìŠ¤ ì„¸ë¼ë…¸), Rosy Park, ë©”ì´í”ŒìŠ¤í† ë¦¬
# - ì–´ë–¤ ë°ì´í„°ì˜ ê°’ì„ ì˜ˆì¸¡í•œë‹¤ê³  í•  ë•Œ, í•˜ë‚˜ì˜ ëª¨ë¸ë§Œ ê°€ì§€ê³  ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ë„ ìˆì§€ë§Œ,  
# ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ì¡°í™”ë¡­ê²Œ í•™ìŠµ(Ensemble Learning)ì‹œì¼œ ê·¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë“¤ì„ ì´ìš©í•œë‹¤ë©´ ë” ì •í™•í•œ ì˜ˆì¸¡ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.
# - ì—¬ëŸ¬ ê°œì˜ ë¶„ë¥˜ê¸°(Classifier)ë¥¼ ìƒì„±í•˜ê³  ê·¸ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ 1ê°œì˜ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ë” ì •í™•í•˜ê³  ì‹ ë¢°ì„± ë†’ì€ ì˜ˆì¸¡ì„ ë„ì¶œí•˜ëŠ” ê¸°ë²•ì´ë‹¤.
# - ê°•ë ¥í•œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì•½í•œ ëª¨ë¸ ì—¬ëŸ¬ ê°œë¥¼ ì¡°í•©í•˜ì—¬ ë” ì •í™•í•œ ì˜ˆì¸¡ì— ë„ì›€ì„ ì£¼ëŠ” ë°©ì‹ì´ë‹¤.
# - ì•™ìƒë¸” í•™ìŠµì˜ ì£¼ìš” ë°©ë²•ì€ ë°°ê¹…(Bagging)ê³¼ ë¶€ìŠ¤íŒ…(Boosting)ì´ë‹¤. 
# <div style="display: flex;">
#     <div>
#         <img src="./images/ensemble_learning01.png" width="400" style="margin-top:20px; margin-left:0">
#     </div>
#     <div>
#         <img src="./images/ensemble_learning02.png" width="400" style="margin-left:0">
#     </div>
# </div>

# ### ì•™ìƒë¸”ì˜ ìœ í˜•
# #### ë³´íŒ…(Voting)
# - "í•˜ë‚˜ì˜ ë°ì´í„° ì„¸íŠ¸"ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ê°€ì§„ ë¶„ë¥˜ê¸°ë¥¼ ê²°í•©í•˜ëŠ” ë°©ì‹ì´ë‹¤.
# - ì„œë¡œ ë‹¤ë¥¸ ë¶„ë¥˜ê¸°ë“¤ì— "ë™ì¼í•œ ë°ì´í„° ì„¸íŠ¸"ë¥¼ ë³‘ë ¬ë¡œ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ê³ , ì´ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì‚°ì¶œí•´ë‚´ëŠ” ë°©ì‹ì„ ë§í•œë‹¤.  
# 
# > ##### 1. í•˜ë“œ ë³´íŒ…(Hard Voting)  
# > -  ê° ë¶„ë¥˜ê¸°ê°€ ë§Œë“  ì˜ˆì¸¡ê°’ì„ ë‹¤ìˆ˜ê²°ë¡œ íˆ¬í‘œí•´ì„œ ê°€ì¥ ë§ì€ í‘œë¥¼ ì–»ì€ ì˜ˆì¸¡ê°’ì„ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ë³´íŒ… ë°©ì‹ì„ ë§í•œë‹¤.
# <img src="./images/hard_voting.png" width="420" style="margin-left:0">
#   
# > ##### 2. ì†Œí”„íŠ¸ ë³´íŒ…(Soft Voting)
# > - ê° ë¶„ë¥˜ê¸°ê°€ ì˜ˆì¸¡í•œ íƒ€ê²Ÿë³„ í™•ë¥ ì„ í‰ê· ë‚´ì–´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ íƒ€ê²Ÿì„ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë„ì¶œí•œë‹¤.
# <img src="./images/soft_voting.png" width="440" style="margin-left:-5px">  
# > ##### ğŸ† í•˜ë“œë³´íŒ…ê³¼ ì†Œí”„íŠ¸ë³´íŒ… ì¤‘ ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•œ ëª¨ë¸ë¡œ ì„ íƒí•˜ë©´ ëœë‹¤.
# #### ë°°ê¹…(Bagging, Bootstrap Aggregation)
# - í•˜ë‚˜ì˜ ë°ì´í„° ì„¸íŠ¸ì—ì„œ "ì—¬ëŸ¬ ë²ˆ ì¤‘ë³µì„ í—ˆìš©í•˜ë©´ì„œ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëœë¤í•˜ê²Œ ë½‘ì€ ë’¤(Bootstrap)"  
# í•˜ë‚˜ì˜ ì˜ˆì¸¡ê¸° ì—¬ëŸ¬ ê°œë¥¼ ë³‘ë ¬ë¡œ í•™ìŠµì‹œì¼œ ê²°ê³¼ë¬¼ì„ ì§‘ê³„(Aggregration)í•˜ëŠ” ë°©ë²•ì´ë‹¤.
# - Voting ë°©ì‹ê³¼ ë‹¬ë¦¬ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì˜ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ê³  í›ˆë ¨ ì„¸íŠ¸ë¥¼ ë¬´ì‘ìœ„ë¡œ êµ¬ì„±í•˜ì—¬ ê°ê¸° ë‹¤ë¥´ê²Œ(ë…ë¦½ì ìœ¼ë¡œ, ë³‘ë ¬ë¡œ) í•™ìŠµì‹œí‚¨ë‹¤.  
# - í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë”ë¼ë„ ì¶©ë¶„í•œ í•™ìŠµíš¨ê³¼ë¥¼ ì£¼ì–´ ê³¼ì í•©ë“±ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.
# - ë°°ê¹…ë°©ì‹ì„ ì‚¬ìš©í•œ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì´ ë°”ë¡œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
# 
# ğŸ“Œë¶€íŠ¸ìŠ¤íŠ¸ë©(bootstrap)ì€ í†µê³„í•™ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´ë¡œ, random samplingì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì¼ì»«ëŠ” ë§ì´ë‹¤.
# <img src="./images/voting_bagging.png" width="500" style="margin-top: 20px; margin-left:0">  
# 
# ---
# > ğŸš©ì •ë¦¬.  
# <strong style="color: purple">ë³´íŒ…(Voting)</strong>ê³¼ <strong style="color: green">ë°°ê¹…(Bagging)</strong>ì€  
# <strong>ì—¬ëŸ¬ ê°œì˜ ë¶„ë¥˜ê¸°ê°€ í•˜ë‚˜ì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ í›ˆë ¨</strong>í•œ ë’¤ íˆ¬í‘œë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°ì •í•œë‹¤ëŠ” ê³µí†µì ì´ ìˆì§€ë§Œ,  
# ë³´íŒ…ì€ ê°ê° <strong style="color: purple">ë™ì¼í•œ ë°ì´í„° ì„¸íŠ¸, ë‹¤ë¥¸ ë¶„ë¥˜ê¸°</strong> , ë°°ê¹…ì€ <strong style="color: green">ê°ê°ì˜ ë°ì´í„° ì„¸íŠ¸(ì¤‘ë³µ í—ˆìš©), ê°™ì€ ë¶„ë¥˜ê¸°</strong>ë¥¼ ì‚¬ìš©í•œë‹¤.
# ---
# #### ë¶€ìŠ¤íŒ…(Boosting)
# - ì´ì „ ë¶„ë¥˜ê¸°ì˜ í•™ìŠµ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ë‹¤ìŒ ë¶„ë¥˜ê¸°ì˜ í•™ìŠµ ë°ì´í„°ì˜ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•´ "ìˆœì°¨ì ìœ¼ë¡œ" í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë°©ë²•ì´ë‹¤.
# - ì´ì „ ë¶„ë¥˜ê¸°ë¥¼ ê³„ì† ê°œì„ í•´ ë‚˜ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ê³ , ì˜¤ë‹µì— ëŒ€í•´ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ë¯€ë¡œ ì •í™•ë„ê°€ ë†’ê²Œ ë‚˜íƒ€ë‚œë‹¤.  
# - ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜(outlier)ì— ì·¨ì•½í•  ìˆ˜ ìˆë‹¤.
# <img src="./images/boosting01.png" width="600" style="margin-top: 20px; margin-left:0">  
# 
# > ##### 1. Adaboost(Adaptive boosting)
# > - ë¶€ìŠ¤íŒ…ì—ì„œ ê°€ì¥ ê¸°ë³¸ ê¸°ë²•ì´ë©°,  
# ê²°ì • íŠ¸ë¦¬ì™€ ë¹„ìŠ·í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì§€ë§Œ ë»—ì–´ë‚˜ê°€(tree)ì§€ ì•Šê³  í•˜ë‚˜ì˜ ì¡°ê±´ì‹ë§Œ ì‚¬ìš©(stump)í•˜ì—¬ ê²°ì •í•œë‹¤.
# > - ì—¬ëŸ¬ ê°œì˜ stumpë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ë¥¼ Forest of stumpsë¼ê³  í•œë‹¤.  
# ğŸ“Œ stumpë€, "ë‚˜ë¬´ì˜ ì˜ë¦¬ê³  ë‚¨ì€ ë¶€ë¶„"ì´ë¼ëŠ” ëœ»ì´ë©°, ì¡°ê±´ì‹ í•˜ë‚˜ì™€ ë‘ ê°ˆë˜ì˜ ì°¸, ê±°ì§“ ë¦¬í”„ ë…¸ë“œê°€ ìˆëŠ” í˜•íƒœì´ë‹¤.
# > - íŠ¸ë¦¬ì™€ ë‹¤ë¥´ê²Œ, ìŠ¤í…€í”„ëŠ” ë‹¨ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì•½í•œ í•™ìŠµê¸°(weak learner)ì´ë‹¤.
# <img src="./images/boosting02.png" width="600" style="margin-top: 20px; margin-left:0">  
# > - ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í° ìŠ¤í…€í”„ë¥¼ Amount of Sayê°€ ë†’ë‹¤(ê°€ì¤‘ì¹˜ê°€ ë†’ë‹¤)ê³  í•œë‹¤. 
# > - ê° ìŠ¤í…€í”„ì˜ errorëŠ” ë‹¤ìŒ ìŠ¤í…€í”„ì˜ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ì¤„ì¤„ì´ ë§ˆì§€ë§‰ ìŠ¤í…€í”„ê¹Œì§€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
# > - ê° ìŠ¤í…€í”„ì˜ Amount of Sayë¥¼ ìˆ˜ì¹˜ë¡œ êµ¬í•œ ë’¤ ì—¬ëŸ¬ ìŠ¤í…€í”„ì˜ Amount of Sayë¥¼ í•©ì¹˜ë©´, Total Amount of Sayê°€ ë‚˜ì˜¨ë‹¤.  
# ì´ë¥¼ í†µí•´ ìµœì¢… ë¶„ë¥˜ê°€ ëœë‹¤.  
# > - í•˜ë‚˜ì˜ ìŠ¤í…€í”„ëŠ” ì•½í•œ í•™ìŠµê¸°ì´ì§€ë§Œ ì—¬ëŸ¬ ìŠ¤í…€í”„ë¥¼ ëª¨ìœ¼ë©´ ê°•í•œ í•™ìŠµê¸°ê°€ ëœë‹¤.
# <img src="./images/stump.jpg" width="350" style="margin: 20px; margin-left:-10px">  
# 
# >> ##### Amount of Say
# >> - Total Errorê°€ 0ì´ë©´ Amount of SayëŠ” êµ‰ì¥íˆ í° ì–‘ìˆ˜ì´ê³ , Total Errorê°€ 1ì´ë©´ Amount of SayëŠ” êµ‰ì¥íˆ ì‘ì€ ìŒìˆ˜ê°€ ëœë‹¤.  
# >> - Total Errorê°€ 0ì´ë©´ í•­ìƒ ì˜¬ë°”ë¥¸ ë¶„ë¥˜ë¥¼ í•œë‹¤ëŠ” ëœ»ì´ê³ , 1ì´ë©´ í•­ìƒ ë°˜ëŒ€ë¡œ ë¶„ë¥˜ë¥¼ í•œë‹¤ëŠ” ëœ»ì´ë©°, Total Errorê°€ 0.5ì¼ ë•ŒëŠ” Amount of SayëŠ” 0ì´ë‹¤. 0ê³¼ 1ì´ ë°˜ë°˜ì´ê¸° ë•Œë¬¸ì—, ë¶„ë¥˜ê¸°ë¡œì„œ ë™ì „ ë˜ì§€ê¸°ì™€ ê°™ì´ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ëœë¤ìœ¼ë¡œ íŒë‹¨í•œë‹¤.
# <div style="width: 70%; height: 260px; display: flex; margin: 20px; margin-left: 100px">
#     <div>
#         <img src="./images/amount_of_say01.png" width="300">  
#     </div>
#     <div>
#         <img src="./images/amount_of_say02.png" width="250">  
#     </div>
# </div>  
# 
# > ##### 2. GBM(Gradient Boost Machine)
# > - Adaboostì™€ ìœ ì‚¬í•˜ì§€ë§Œ, ì—ëŸ¬ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì„ ì´ìš©í•œë‹¤.  
# > - GBMì€ ê³¼ì í•©ì—ë„ ê°•í•œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë³‘ë ¬ ì²˜ë¦¬ê°€ ë˜ì§€ ì•Šì•„ ìˆ˜í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.  
# ğŸ“Œ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì´ë€, ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ Loss funtionì„ ìµœì†Œí™”í•  ìˆ˜ ìˆëŠ” ìµœì†Œê°’ê¹Œì§€ ë°˜ë³µí•´ì„œ ì ì°¨ í•˜ê°•í•˜ë©° ì°¾ì•„ë‚˜ê°€ëŠ” ê¸°ë²•ì´ë‹¤.
# <div style="width: 70%; display: flex; margin-top: 20px; margin-left: 40px">
#     <div>
#         <img src="./images/gradient_boost01.png" width="400">  
#     </div>
#     <div>
#         <img src="./images/gradient_boost02.png" width="380">  
#     </div>
# </div>  
# 
# >> ##### ì†ì‹¤ í•¨ìˆ˜(Loss function) ë˜ëŠ” ë¹„ìš© í•¨ìˆ˜(Cost function)  
# >> - ì˜ˆì¸¡ ê°’ì´ ì‹¤ì œ ê°’ê³¼ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì´ë©°, ëª¨ë¸ ì„±ëŠ¥ì˜ ì¢‹ì§€ ì•ŠìŒì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì´ë‹¤.
# >> - ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ë²—ì–´ë‚œ ê±°ë¦¬ë¥¼ ì¢…í•©í•˜ì—¬, ì´ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
# <img src="./images/loss_function.png" width="400" style="margin-top: 20px; margin-left:0">  
# 
# > - ëª¨ë¸ Aë¥¼ í†µí•´ yë¥¼ ì˜ˆì¸¡í•˜ê³  ë‚¨ì€ ì”ì°¨(residual)ë¥¼ ë‹¤ì‹œ Bë¼ëŠ” ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡í•˜ê³  A+B ëª¨ë¸ì„ í†µí•´ yë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì´ë‹¤.
# > - ì”ì°¨ë¥¼ ê³„ì† ì¤„ì—¬ë‚˜ê°€ë©°, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì˜ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ ëœë‹¤.
# > -  ì”ì°¨ë¥¼ ê³„ì† ì¤„ì´ë‹¤ë³´ë©´ ë³µì¡ë„ê°€ ì¦ê°€í•˜ì—¬ ê³¼ì í•©ì´ ì¼ì–´ë‚  ìˆ˜ë„ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.  
# ë”°ë¼ì„œ ì‹¤ì œë¡œ GBMì„ ì‚¬ìš©í•  ë•ŒëŠ” ìˆ˜ì¤€ ë†’ì€ Feature engineeringì„ í•´ì„œ ë” ìµœì í™”í•˜ëŠ” ê²ƒì´ ë³´í¸ì ì´ë‹¤.
# > - ğŸ“Œì”ì°¨ë€(residual), ì‹¤ì œ íƒ€ê²Ÿê°’ ì—ì„œ - Aëª¨ë¸ì˜ ì˜ˆì¸¡ í‰ê· ê°’ì„ ëº€ ê°’ì´ë‹¤. ì¦‰, ì—ëŸ¬ì˜ ë¹„ìœ¨ì´ë‹¤.
# > - í•™ìŠµë¥ (learning rate)ì´ ë†’ì„ ìˆ˜ë¡ ë¹ ë¥´ê²Œ ëª¨ë¸ì˜ ì¹˜ìš°ì¹¨(bias, ë°”ì´ì–´ìŠ¤)ì„ ì¤„ì—¬ë‚˜ê°€ì§€ë§Œ, í•™ìŠµë¥ ì´ ì ìœ¼ë©´ ë””í…Œì¼í•œ ë¶€ë¶„ì„ ë†“ì¹  ìˆ˜ ìˆë‹¤.
# <img src="./images/gradient_boost03.png" width="700" style="margin-top: 20px; margin-left:0">  
# 
# > ##### 3. XGBoost(eXtra Gradient Boost)  
# > - íŠ¸ë¦¬ ê¸°ë°˜ì˜ ì•™ìƒë¸” í•™ìŠµì—ì„œ ê°€ì¥ ê°ê´‘ë°›ê³  ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì´ë©°, ë¶„ë¥˜ì— ìˆì–´ì„œ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ë³´ë‹¤ ë›°ì–´ë‚œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.
# > - GBMì— ê¸°ë°˜í•˜ê³  ìˆì§€ë§Œ ë³‘ë ¬ CPU í™˜ê²½ì—ì„œ ë³‘ë ¬ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ GBMë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµì„ ì™„ë£Œí•  ìˆ˜ ìˆë‹¤.
# > - í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ë¶„í•  ê¹Šì´ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆì§€ë§Œ, tree pruning(ê°€ì§€ì¹˜ê¸°)ìœ¼ë¡œ ë” ì´ìƒ ê¸ì • ì´ë“ì´ ì—†ëŠ” ë¶„í• ì„ ê°€ì§€ì¹˜ê¸° í•´ì„œ ë¶„í•  ìˆ˜ë¥¼ ë” ì¤„ì´ëŠ” ì¶”ê°€ì ì¸ ì¥ì ì„ ê°€ì§€ê³  ìˆë‹¤.
# > - ì‚¬ì´í‚·ëŸ°ì˜ ê¸°ë³¸ Estimatorë¥¼ ê·¸ëŒ€ë¡œ ìƒì†í•˜ì—¬ ë§Œë“¤ì—ˆê¸° ë•Œë¬¸ì— fit()ê³¼ predict()ë§Œìœ¼ë¡œ í•™ìŠµê³¼ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ë‹¤.
# <img src="./images/xgboost.png" width="900" style="margin-top: 20px; margin-left:-20px">  
# 
# >> ##### ì¡°ê¸° ì¤‘ë‹¨ ê¸°ëŠ¥(Early Stopping)
# >> - íŠ¹ì • ë°˜ë³µ íšŸìˆ˜ ë§Œí¼ ë” ì´ìƒ ì†ì‹¤í•¨ìˆ˜ê°€ ê°ì†Œí•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜í–‰ì„ ì¢…ë£Œí•  ìˆ˜ ìˆë‹¤.
# >> - í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ìµœì í™” íŠœë‹ ì‹œ ì ì ˆí•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
# >> - ë°˜ë³µ íšŸìˆ˜ë¥¼ ë„ˆë¬´ ë‚®ê²Œ ì„¤ì •í•˜ë©´, ìµœì í™” ì „ì— í•™ìŠµì´ ì¢…ë£Œë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¡°ì‹¬í•´ì•¼ í•œë‹¤.
# <img src="./images/early_stopping.png" width="400" style="margin-top: 20px; margin-left:-20px">  
# 
# > ##### 4. LightGBM(Light Gradient Boosting Machine)
# > - XGBoostì˜ í–¥ìƒëœ ë²„ì „ìœ¼ë¡œì„œ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆœìœ„ ì§€ì •, ë¶„ë¥˜ ë° ê¸°íƒ€ ì—¬ëŸ¬ ê¸°ê³„ í•™ìŠµ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
# > - ê¸°ì¡´ ë¶€ìŠ¤íŒ… ë°©ì‹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ê°ê°ì˜ ìƒˆë¡œìš´ ë¶„ë¥˜ê¸°ê°€ ì´ì „ íŠ¸ë¦¬ì˜ ì”ì°¨ë¥¼ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì´ í–¥ìƒë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê²°í•© ë˜ê³ ,  
# ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ê°€ëœ íŠ¸ë¦¬ëŠ” ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ì—¬ ê°•ë ¥í•œ ë¶„ë¥˜ê¸°ê°€ ë  ìˆ˜ ìˆë‹¤.
# > -  XGBoostì™€ ë‹¬ë¦¬ GOSS ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ì„±ì¥ì‹œí‚¨ë‹¤. ì¦‰, ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì€ ë ˆë²¨ ë‹¨ìœ„ë¡œ ì„±ì¥ì‹œí‚¤ì§€ë§Œ, LightGBMì€ íŠ¸ë¦¬ë¥¼ ë¦¬í”„ ë‹¨ìœ„ë¡œ ì„±ì¥ì‹œí‚¨ë‹¤.  
# > - ì¸ì½”ë”©ì„ ë”°ë¡œ í•  í•„ìš” ì—†ì´ ì¹´í…Œê³ ë¦¬í˜• featureë¥¼ ìµœì ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì´ì— ë”°ë¥¸ ë…¸ë“œ ë¶„í• ì„ ìˆ˜í–‰í•œë‹¤. astype('category')ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” LightGBMì—ì„œ ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ì˜ ë‹¤ì–‘í•œ ì¸ì½”ë”© ë°©ì‹ë³´ë‹¤ ì›”ë“±í•˜ë‹¤.
# > - <strong style="color: orange">XGBoostëŠ” ì†Œê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ë” ì í•©í•˜ë©° ëŒ€ëŸ‰ì˜ ë°ì´í„°ì— ê³¼ì í•©(overfitting)ë  ìœ„í—˜ì´ ìˆëŠ” ë°˜ë©´</strong> <strong style="color: purple">LightGBMì€ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ë” ì í•©í•˜ë©° ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œëŠ” í•™ìŠµì´ ëœ ë˜ì–´ ìˆëŠ” ê³¼ì†Œì í•©(underfitting)ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.</strong>
# <img src="./images/lightGBM01.png" width="600" style="margin-top: 20px; margin-left:-20px">  
# 
# >> ##### Gradient-based One-Side Sampling (GOSS)
# >> - ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œì„œ, ëœ í•™ìŠµëœ ë°ì´í„°(gradientê°€ í° ë°ì´í„°)ê°€ ì˜í–¥ë ¥ì´ í¬ë‹¤ëŠ” ê°€ì •ìœ¼ë¡œ ì¶œë°œí•œë‹¤.  
# >> - ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¤„ì¼ ë•Œ, gradientê°€ í° ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê³ , gradientê°€ ì‘ì€ ë°ì´í„°ë“¤ì„ ë¬´ì‘ìœ„ë¡œ dropí•œë‹¤.
# >> - ê· ë“±í•˜ê²Œ ëœë¤ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ê²ƒ ë³´ë‹¤ ìœ„ ë°©ì‹ì´ ë” ì •í™•í•œ ì •ë³´ í•™ìŠµì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.
# >> - ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì¼ë¶€ ë°ì´í„°ë§Œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì†ë„ê°€ êµ‰ì¥íˆ ë¹ ë¥´ë‹¤.
# >> ##### GOSS ë…¼ë¬¸: https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf
# >> ##### GOSS ë…¼ë¬¸ í•´ì„ ì˜ìƒ: https://www.youtube.com/watch?v=yZGlt3rGtVs
# <div style="display: flex; margin-left: 80px; margin-top:-30px">
#     <div>
#         <img src="./images/lightGBM02.png" width="400"> 
#     </div>
#     <div>
#         <img src="./images/goss01.png" width="300">  
#     </div>
# </div>
# 
# > ğŸš© ê²°ê³¼ì˜ ì •í™•ì„±ì„ ë†’ì´ê³  ê°•í™”í•˜ëŠ” ê¸°ëŠ¥ ë•ë¶„ì— LightGBMì€ í•´ì»¤í†¤ ë° ë¨¸ì‹ ëŸ¬ë‹ ëŒ€íšŒëŠ” ë¬¼ë¡  Kaggle ëŒ€íšŒì—ì„œë„   
# ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

# ### ë³´íŒ…(Voting)
# 
# ##### VotingClassifier(n_estimators, voting)
# 
# ###### n_estimators  
# - ì¶”ê°€í•  ëª¨ë¸ ê°ì²´ë¥¼ listí˜•íƒœë¡œ ì „ë‹¬í•œë‹¤.
# - ì˜ˆì‹œ) [('DTC',grid_dt_classifier),('SVC',grid_sv_classifier), ('KNN', grid_knn_classifier)]
# 
# ###### voting
# - soft, hard ì¤‘ ì„ íƒí•œë‹¤.
# - default: 'hard'

# ##### ìœ ë°©ì•” ì˜ˆì¸¡ - ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„° ì„¸íŠ¸

# In[ ]:


import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
cancer_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
cancer_df['target'] = cancer.target
cancer_df


# In[ ]:


cancer_df.info()


# In[ ]:


cancer_df.isna().sum()


# In[ ]:


cancer_df['target'].value_counts()


# In[ ]:


import seaborn as sns
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(7, 5))
correlation_matrix = cancer_df.corr()
heatmap = sns.heatmap(correlation_matrix, cmap='Oranges')
heatmap.set_title("Correlation")


# In[ ]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import GridSearchCV, train_test_split

# í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹
dt_params = {'max_depth': [5, 6, 7], 'min_samples_split': [7, 8, 9]}
svm_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
             'gamma': [0.001, 0.01, 0.1, 1, 10, 100],
             'kernel': ['linear', 'rbf']}
knn_params = {'n_neighbors': [3, 5, 7, 9, 11]}

grid_dt_classifier = GridSearchCV(DecisionTreeClassifier(), param_grid=dt_params, cv=10, refit=True, return_train_score=True, n_jobs=4, error_score='raise')
# ì†Œí”„íŠ¸ ë³´íŒ…ì—ì„œëŠ” ê° ê²°ì • í´ë˜ìŠ¤ë³„ í™•ë¥ ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì—, SVCì— probabilityë¥¼ Trueë¡œ í•˜ì—¬
# predict_proba()ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤(í—ˆì€ìƒ ë„ì›€).
grid_svc_classifier = GridSearchCV(SVC(probability=True), param_grid=svm_params, cv=5, refit=True, return_train_score=True, n_jobs=4, error_score='raise')
# KNNì—ì„œ Flagì˜¤ë¥˜ ë°œìƒ
# Series íƒ€ì…ì˜ í›ˆë ¨ ë°ì´í„°ì—ëŠ” flags ì†ì„±ì´ ì—†ê¸° ë•Œë¬¸ì—, numpyë¡œ ë³€ê²½í•œ ë’¤ í›ˆë ¨ì‹œì¼œì•¼ í•œë‹¤.
grid_knn_classifier = GridSearchCV(KNeighborsClassifier(), param_grid=knn_params, cv=10, refit=True, return_train_score=True, n_jobs=4, error_score='raise')

# ê°œë³„ ëª¨ë¸ì„ "í•˜ë“œ" ë³´íŒ… ê¸°ë°˜ì˜ ì•™ìƒë¸” ëª¨ë¸ë¡œ êµ¬í˜„í•œ ë¶„ë¥˜ê¸°
# ì˜¤ì°¨ í–‰ë ¬
# [[40  2]
#  [ 3 69]]
# ì •í™•ë„: 0.9561, ì •ë°€ë„: 0.9718, ì¬í˜„ìœ¨: 0.9583, F1:0.9650, AUC:0.9554

# voting_classifier = VotingClassifier(estimators=[('DTC', grid_dt_classifier)
#                                                  , ('SVC', grid_svc_classifier)
#                                                  , ('KNN', grid_knn_classifier)]
#                                      , voting='hard')


# ê°œë³„ ëª¨ë¸ì„ "ì†Œí”„íŠ¸" ë³´íŒ… ê¸°ë°˜ì˜ ì•™ìƒë¸” ëª¨ë¸ë¡œ êµ¬í˜„í•œ ë¶„ë¥˜ê¸°
# ì˜¤ì°¨ í–‰ë ¬
# [[37  5]
#  [ 1 71]]
# ì •í™•ë„: 0.9474, ì •ë°€ë„: 0.9342, ì¬í˜„ìœ¨: 0.9861, F1:0.9595, AUC:0.9335

voting_classifier = VotingClassifier(estimators=[('DTC', grid_dt_classifier)
                                                 , ('SVC', grid_svc_classifier)
                                                 , ('KNN', grid_knn_classifier)]
                                     , voting='soft')

# ë°ì´í„° ì„¸íŠ¸ ë¶„ë¦¬
features, targets = cancer.data, cancer.target
X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.2)

# VotingClassifier í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€
voting_classifier.fit(X_train, y_train)


# In[ ]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test , prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test , prediction)
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test , prediction)
#     F1 score
    f1 = f1_score(y_test, prediction)
#     ROC-AUC
    roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy , precision ,recall, f1, roc_auc))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues, normalize=normalize)
            disp.ax_.set_title(title)
        plt.show()


# In[ ]:


prediction = voting_classifier.predict(X_test)
get_evaluation(y_test, prediction, voting_classifier, X_test)


# In[ ]:


# ê°œë³„ ëª¨ë¸ì˜ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€.
classifiers = [grid_dt_classifier, grid_svc_classifier, grid_knn_classifier]
for classifier in classifiers:
    classifier.fit(X_train , y_train)
    prediction = classifier.predict(X_test)
    class_name= classifier.best_estimator_.__class__.__name__
    print(f'# {class_name}')
    get_evaluation(y_test, prediction, classifier, X_test)


# ### ë°°ê¹…(Bagging) - ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)
# 
# #### RandomForestClassifier(n_estimators, min_samples_split, min_samples_leaf, n_jobs)
# 
# ###### n_estimators  
# - ìƒì„±í•  treeì˜ ê°œìˆ˜ë¥¼ ì‘ì„±í•œë‹¤.
# - default: 50
# 
# ###### min_samples_split
# - ë¶„í•  í•  ìˆ˜ ìˆëŠ” ìƒ˜í”Œ ìˆ˜ì´ë‹¤.
# 
# ##### min_samples_leaf
# - ë¶„í• í–ˆì„ ë•Œ leafì˜ ìƒ˜í”Œ ìˆ˜ì´ë‹¤.

# In[ ]:


import pandas as pd

car_df = pd.read_csv('./datasets/car.csv')
car_df


# In[ ]:


car_df.info()


# In[ ]:


from sklearn.preprocessing import LabelEncoder

obj_columns = ['Price', 'Main_cost', 'Doors', 'Persons', 'Lug_cap', 'Safety', 'Decision']
encoders = []
for column in obj_columns:
    encoder = LabelEncoder()
    car_df[column] = encoder.fit_transform(car_df[column].tolist())
    encoders.append(encoder)
    print(encoder.classes_)


# In[ ]:


car_df['Price'].value_counts()


# In[ ]:


import seaborn as sns
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(5, 4))
correlation_matrix = car_df.corr()
sns.heatmap(correlation_matrix, cmap="Purples")


# In[ ]:


car_df.iloc[:, 1:].hist(figsize=(10, 8))


# In[ ]:


# car_df['Decision'].value_counts()

# dicision_0 = car_df[car_df['Decision'] == 0]
# dicision_1 = car_df[car_df['Decision'] == 1]
# dicision_2 = car_df[car_df['Decision'] == 2].sample(384)
# dicision_3 = car_df[car_df['Decision'] == 3]

# banlance_car_df = pd.concat([dicision_0, dicision_1, dicision_2, dicision_3])
# banlance_car_df['Decision'].value_counts()


# In[ ]:


from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'max_depth': [4, 6, 8, 10, 12],
    'min_samples_split': [6, 12, 18, 24],
    'min_samples_leaf': [4, 8, 16]
}

random_forest_classifier = RandomForestClassifier(n_estimators=100)

# features, targets = banlance_car_df.iloc[:, 1:], banlance_car_df.Price
features, targets = car_df.iloc[:, 1:], car_df.Price

X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.3)

grid_random_forest = GridSearchCV(random_forest_classifier, param_grid=param_grid, cv=10, n_jobs=4)

grid_random_forest.fit(X_train, y_train)


# In[ ]:


# DataFrameìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_random_forest.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']].sort_values(by='rank_test_score')


# In[ ]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test , prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test , prediction, average='macro')
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test , prediction, average='macro')
#     F1 score
    f1 = f1_score(y_test, prediction, average='macro')
#     ROC-AUC
#     roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}'.format(accuracy , precision ,recall, f1))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues, normalize=normalize)
            disp.ax_.set_title(title)
        plt.show()


# In[ ]:


prediction = grid_random_forest.predict(X_test)
get_evaluation(y_test, prediction, grid_random_forest, X_test)


# ##### car.csv ë°ì´í„° ì„¸íŠ¸ëŠ” ì „ì²´ì ìœ¼ë¡œ ë¶„í¬ëŠ” ê´œì°®ì§€ë§Œ, ë°ì´í„°ì˜ ì–‘ì´ ë¶€ì¡±í•˜ì—¬ Under fittingì´ ë°œìƒí•œë‹¤.

# ### ë¶€ìŠ¤íŒ…(Boosting) - ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸(Adaptive Boost)
# 
# #### AdaBoostClassifier(base_estimators, n_estimators, learning_rate)
# 
# ###### base_estimators  
# - í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì„ íƒí•œë‹¤.  
# - default: DecisionTreeClassifier(max_depth = 1)
# 
# ###### n_estimators
# - ìƒì„±í•  ì•½í•œ í•™ìŠµê¸°ì˜ ê°œìˆ˜ë¥¼ ì§€ì •í•œë‹¤.  
# - default : 50
# 
# ##### learning_rate
# - í•™ìŠµì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì ìš©í•˜ëŠ” í•™ìŠµë¥ (0~1 ì‚¬ì´ì˜ ê°’)ì´ë©°, ì•½í•œ í•™ìŠµê¸°ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì˜¤ë¥˜ê°’ì„ ë³´ì •í•´ë‚˜ê°ˆ ë•Œ ì ìš©í•˜ëŠ” ê³„ìˆ˜ì´ë‹¤.  
# - ë‚®ì€ë§Œí¼ ìµœì†Œ ì†ì‹¤ê°’ì„ ì°¾ì•„ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë†’ì•„ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ê·¸ ë§Œí¼ ë§ì€ ìˆ˜ì˜ íŠ¸ë¦¬ê°€ í•„ìš”í•˜ê³  ì‹œê°„ì´ ë§ì´ ì†Œìš”ëœë‹¤.  
# - default : 1

# In[1]:


import pandas as pd
water_df = pd.read_csv("./datasets/water_potability.csv")
water_df


# In[2]:


water_df.info()


# In[3]:


water_df.isna().sum()


# In[4]:


water_df.iloc[:, :-1].hist(figsize=(10, 20), bins=100)


# In[5]:


from sklearn.preprocessing import StandardScaler

scale = StandardScaler()
water_scaled = scale.fit_transform(water_df)


# In[6]:


water_scaled_df = pd.DataFrame(water_scaled, columns=water_df.columns)
water_scaled_df[~water_scaled_df['Solids'].between(-1.96, 1.96)]


# In[7]:


water_scaled_df = water_scaled_df[water_scaled_df.Solids.between(-1.96, 1.96)]
water_scaled_df


# In[8]:


water_scaled_df.iloc[:, :-1].hist(figsize=(10, 20), bins=100)


# In[9]:


water_df = water_df.iloc[water_scaled_df.index, :]
water_df


# In[10]:


water_df = water_df.reset_index(drop=True)
water_df


# In[11]:


water_df.isna().sum()
water_df.Sulfate = water_df.Sulfate.fillna(0)
water_df


# In[12]:


water_df.isna().sum()


# In[13]:


water_df.Trihalomethanes = water_df.Trihalomethanes.fillna(0)
water_df.isna().sum()


# In[14]:


water_df.ph = water_df.ph.fillna(water_df.ph.median())
water_df.isna().sum()


# In[15]:


water_df.duplicated().sum()


# In[16]:


from sklearn.preprocessing import MinMaxScaler

features, targets = water_df.iloc[:, :-1], water_df.Potability

water_scaled = MinMaxScaler().fit_transform(features)
water_scaled_df = pd.DataFrame(water_scaled, columns=features.columns)
water_scaled_df


# In[17]:


water_scaled_df['Potability'] = water_df['Potability']
water_scaled_df


# In[18]:


water_scaled_df.Potability.value_counts()


# In[19]:


target_0 = water_scaled_df[water_scaled_df.Potability == 0].sample(1211)
target_1 = water_scaled_df[water_scaled_df.Potability == 1]

balance_water_df = pd.concat([target_0, target_1])
balance_water_df.Potability.value_counts()


# In[20]:


balance_water_df.reset_index(drop=True, inplace=True)


# In[21]:


balance_water_df.isna().sum()
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import AdaBoostClassifier

param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.3, 0.5, 0.7]
}

features, targets = balance_water_df.iloc[:, :-1], balance_water_df.Potability
X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.2)

grid_ada_boost = GridSearchCV(AdaBoostClassifier(), param_grid=param_grid, cv=10, n_jobs=4)
grid_ada_boost.fit(X_train, y_train)


# In[22]:


prediction = grid_ada_boost.predict(X_test)


# In[25]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
import matplotlib.pyplot as plt
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test , prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test , prediction)
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test , prediction)
#     F1 score
    f1 = f1_score(y_test, prediction)
#     ROC-AUC
    roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy , precision ,recall, f1, roc_auc))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues, normalize=normalize)
            disp.ax_.set_title(title)
        plt.show()


# In[26]:


get_evaluation(y_test, prediction, grid_ada_boost, X_test)


# In[27]:


def get_evaluation_by_thresholds(y_test, prediction_proba_class1, thresholds):
    
    for threshold in thresholds:
        binarizer = Binarizer(threshold=threshold).fit(prediction_proba_class1) 
        custom_prediction = binarizer.transform(prediction_proba_class1)
        print('ì„ê³—ê°’:', threshold)
        get_evaluation(y_test, custom_prediction)


# In[28]:


from sklearn.preprocessing import Binarizer
from sklearn.metrics import precision_recall_curve

prediction_prob = grid_ada_boost.predict_proba(X_test)
prediction_prob_class1 = prediction_prob[:, 1].reshape(-1, 1)

precision, recall, thresolds = precision_recall_curve(y_test, prediction_prob_class1)

get_evaluation_by_thresholds(y_test, prediction_prob_class1, thresolds)


# In[29]:


prediction = Binarizer(threshold=0.5002843602697008).fit_transform(prediction_prob_class1)
get_evaluation(y_test, prediction, grid_ada_boost, X_test)


# ### ë¶€ìŠ¤íŒ…(Boosting) - GBM(Gradient Boosting Machine)
# 
# #### GradientBoostingClassifier(n_estimators, loss, learning_rate, subsample)
# 
# ##### n_estimators
# - ì•½í•œ í•™ìŠµê¸°ì˜ ê°œìˆ˜ì´ë©°, ê°œìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ì¼ì • ìˆ˜ì¤€ê¹Œì§€ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆì§€ë§Œ ë§ì„ ìˆ˜ë¡ ìˆ˜í–‰ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤.
# - default: 100
# 
# ##### loss
# - ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ì‚¬ìš©í•  ë¹„ìš©í•¨ìˆ˜ë¥¼ ì§€ì •í•œë‹¤.
# - default: 'log_loss'
# > ##### ğŸš©ì¶œì²˜  
# > https://library.virginia.edu/data/articles/understanding-deviance-residuals  
# > https://www.youtube.com/watch?v=lAq96T8FkTw&list=LLypIXWIsUMIMvCa6zQfOZmQ&index=14  
# > ëŸ¬ë‹ë¨¸ì‹ 
# > 1. log_loss(deviance)
# > - ê° ì”ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ í‰ê· ì„ ë‚´ëŠ” ì•Œê³ ë¦¬ì¦˜(ë¡œì§€ìŠ¤í‹± íšŒê·€ ì•Œê³ ë¦¬ì¦˜ ë°©ì‹),   
# ì˜¤ë˜ëœ ë°ì´í„°ì˜ ì˜í–¥ê³¼ ìµœì‹  ë°ì´í„°ì˜ ì˜í–¥ì´ ë¹„ìŠ·í•´ì§.  
# > 2. exponential  
# > - Weightë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ì„ ë„ì…(AdaBoost ì•Œê³ ë¦¬ì¦˜ ë°©ì‹)í•˜ì˜€ìœ¼ë©°,   
# ë°ì´í„°ì˜ ì‹œê°„ íë¦„ì— ë”°ë¼ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì‡ í•˜ë„ë¡ ì„¤ê³„í•œ ì•Œê³ ë¦¬ì¦˜.
# > - ğŸ“Œì§€ìˆ˜ì  ê°ì‡ ë€, ì–´ë–¤ ì–‘ì´ ê·¸ ì–‘ì— ë¹„ë¡€í•˜ëŠ” ì†ë„ë¡œ ê°ì†Œí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, 0~1 ì‚¬ì´ì˜ ê°’ì´ ì œê³±ì´ ë˜ë©´ ë” ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ì˜¤ë˜ëœ ë°ì´í„°ì¼ìˆ˜ë¡ í˜„ì¬ì˜ ê²½í–¥ì„ í‘œí˜„í•˜ëŠ” ë°ì— ë” ì ì€ ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ í•œë‹¤.
# 
# ---
# > - ì‹œê³„ì—´ ë°ì´í„°: exponential, ì¼ë°˜ ë°ì´í„°: deviance
# > - ğŸ“Œì‹œê³„ì—´ ë°ì´í„°ë€, ì¼ì • ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ì¸¡ì •ëœ ë°ì´í„°ì˜ ì‹œê°„ì  ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ë¥¼ ì˜ë¯¸í•œë‹¤.
# 
# ##### learning_rate
# - GBMì´ í•™ìŠµì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì ìš©í•˜ëŠ” í•™ìŠµë¥ ì´ë‹¤.
# - ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ ë‚˜ê°€ëŠ” ë°ì— ì ìš©í•˜ëŠ” ê³„ìˆ˜ì´ë©°, 0~1ì‚¬ì´ë¡œ ê°’ì„ ì§€ì •í•œë‹¤.
# - ë†’ê²Œ ì„¤ì •í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ê°’ì„ ì°¾ì§€ ëª»í•˜ê³  ì§€ë‚˜ì³ë²„ë¦¬ì§€ë§Œ ë¹ ë¥¸ ìˆ˜í–‰ì´ ê°€ëŠ¥í•˜ê³ ,  
# ë‚®ê²Œ ì„¤ì •í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ ê°’ì„ ì°¾ì•„ì„œ ì„±ëŠ¥ì€ ë†’ì•„ì§€ì§€ë§Œ, ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤.
# <img src="./images/learning_rate.png" width="600" style="margin-left: 0">  
# 
# ##### subsample
# - í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì´ë‹¤.
# - default: 1 (100%)
# - ê³¼ì í•© ë°©ì§€ ì‹œ 1ë³´ë‹¤ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤.

# In[1]:


import pandas as pd

car_df = pd.read_csv('./datasets/car.csv')
car_df


# In[2]:


from sklearn.preprocessing import LabelEncoder

obj_columns = ['Price', 'Main_cost', 'Doors', 'Persons', 'Lug_cap', 'Safety', 'Decision']
encoders = []
for column in obj_columns:
    encoder = LabelEncoder()
    car_df[column] = encoder.fit_transform(car_df[column].tolist())
    encoders.append(encoder)
    print(encoder.classes_)


# In[5]:


import time
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import GradientBoostingClassifier

features, targets = car_df.iloc[:, 1:], car_df.Price

X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.2)

start_time = time.time()

param_grid = {
    'n_estimators': [50, 100, 500],
    'learning_rate': [0.3, 0.5, 0.7]
}

grid_gradient_boosting = GridSearchCV(GradientBoostingClassifier(), param_grid=param_grid, cv=3)

grid_gradient_boosting.fit(X_train, y_train)
print(f'GBM ìˆ˜í–‰ ì‹œê°„: {time.time() - start_time}')


# In[6]:


# DataFrameìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_gradient_boosting.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']].sort_values(by='rank_test_score')


# In[9]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
import matplotlib.pyplot as plt
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test , prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test , prediction, average='macro')
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test , prediction, average='macro')
#     F1 score
    f1 = f1_score(y_test, prediction, average='macro')
#     ROC-AUC
#     roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}'.format(accuracy , precision ,recall, f1))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues, normalize=normalize)
            disp.ax_.set_title(title)
        plt.show()


# In[10]:


prediction = grid_gradient_boosting.predict(X_test)
get_evaluation(y_test, prediction, grid_gradient_boosting, X_test)


# ### ë¶€ìŠ¤íŒ…(Boosting) - XGBoost(eXtra Gradient Boost)
# 
# #### XGBClassifier(n_estimators, learning_rate, subsample)
# 
# ##### n_estimators
# - ì•½í•œ í•™ìŠµê¸°ì˜ ê°œìˆ˜ì´ë©°, ê°œìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ì¼ì • ìˆ˜ì¤€ê¹Œì§€ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆì§€ë§Œ ë§ì„ ìˆ˜ë¡ ìˆ˜í–‰ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤.
# - default: 100
# 
# ##### learning_rate
# - GBMì´ í•™ìŠµì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì ìš©í•˜ëŠ” í•™ìŠµë¥ ì´ë‹¤.
# - ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ ë‚˜ê°€ëŠ” ë°ì— ì ìš©í•˜ëŠ” ê³„ìˆ˜ì´ë©°, 0~1ì‚¬ì´ë¡œ ê°’ì„ ì§€ì •í•œë‹¤.
# - ë†’ê²Œ ì„¤ì •í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ê°’ì„ ì°¾ì§€ ëª»í•˜ê³  ì§€ë‚˜ì³ë²„ë¦¬ì§€ë§Œ ë¹ ë¥¸ ìˆ˜í–‰ì´ ê°€ëŠ¥í•˜ê³ ,  
# ë‚®ê²Œ ì„¤ì •í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ ê°’ì„ ì°¾ì•„ì„œ ì„±ëŠ¥ì€ ë†’ì•„ì§€ì§€ë§Œ, ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤.
# <img src="./images/learning_rate.png" width="600" style="margin-left: 0">  
# 
# ##### subsample
# - í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì´ë‹¤.
# - default: 1 (100%)
# - ê³¼ì í•© ë°©ì§€ ì‹œ 1ë³´ë‹¤ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤.
# 
# #### fit(X_train, y_train, eval_set, early_stopping_rounds)
# 
# ##### eval_set
# - ì˜ˆì¸¡ ì˜¤ë¥˜ê°’ì„ ì¤„ì¼ ìˆ˜ ìˆë„ë¡ ë°˜ë³µì í•˜ë©´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ”ë°,   
# ì´ë•Œ í•™ìŠµì€ í•™ìŠµ ë°ì´í„°ë¡œ í•˜ê³  ì˜ˆì¸¡ ì˜¤ë¥˜ê°’ í‰ê°€ëŠ” eval_setë¡œ ì§€ì •ëœ ê²€ì¦ ì„¸íŠ¸ë¡œ í‰ê°€í•œë‹¤.
# 
# ##### early_stopping_rounds
# - ì§€ì •í•œ íšŸìˆ˜ë™ì•ˆ ë” ì´ìƒ ì˜¤ë¥˜ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ë” ì´ìƒ í•™ìŠµì€ ì§„í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

# In[11]:


import xgboost
print(xgboost.__version__)


# ##### ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤(COVID) ì˜ˆì¸¡

# In[27]:


import pandas as pd

corona_df = pd.read_csv('./datasets/corona.csv', low_memory=False)
corona_df.info()


# In[28]:


corona_df = corona_df[~corona_df['Cough_symptoms'].isna()]
corona_df = corona_df[~corona_df['Fever'].isna()]
corona_df = corona_df[~corona_df['Sore_throat'].isna()]
corona_df = corona_df[~corona_df['Headache'].isna()]
corona_df['Age_60_above'] = corona_df['Age_60_above'].fillna('No')
corona_df['Sex'] = corona_df['Sex'].fillna('unknown')
corona_df.isna().sum()


# In[29]:


corona_df['Target'] = corona_df['Corona']
corona_df.drop(columns='Corona', axis=1, inplace=True)


# In[30]:


print(corona_df['Target'].value_counts())
display(corona_df)


# In[31]:


corona_df = corona_df[corona_df['Target'] != 'other']
print(corona_df['Target'].value_counts())


# In[32]:


corona_df = corona_df.drop(columns=['Ind_ID', 'Test_date', 'Sex', 'Age_60_above', 'Known_contact'], axis=1)
corona_df


# ##### ë ˆì´ë¸” ì¸ì½”ë”©

# In[33]:


from sklearn.preprocessing import LabelEncoder

columns = ['Cough_symptoms', 'Fever', 'Sore_throat', 'Shortness_of_breath', 'Headache', 'Target']

for column in columns:
    encoder = LabelEncoder()
    targets = encoder.fit_transform(corona_df[column])
    corona_df.loc[:, column] = targets
    print(f'{column}_classes: {encoder.classes_}')


# In[34]:


corona_df = corona_df.reset_index(drop=True)
corona_df


# In[35]:


corona_df = corona_df.astype('int16')
corona_df.info()


# ##### í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ ë° êµì°¨ ê²€ì¦

# In[36]:


from sklearn.model_selection import GridSearchCV, train_test_split
from xgboost import XGBClassifier

param_grid = {
    'n_estimators': [50, 100, 500],
    'learning_rate': [0.3, 0.5, 0.7]
}

xgb = XGBClassifier()

features, targets = corona_df.iloc[:, :-1], corona_df.Target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.2)

# í•™ìŠµ ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬
X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, stratify=y_train, test_size=0.3)

evals = [(X_val_train, y_val_train), (X_val_test, y_val_test)]

grid_xgb = GridSearchCV(xgb, param_grid, cv=3, refit=True, return_train_score=True, n_jobs=-1, error_score='raise')
grid_xgb.fit(X_train, y_train, early_stopping_rounds=50, eval_set=evals)


# In[37]:


# DataFrameìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_xgb.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']].sort_values(by='rank_test_score')


# In[38]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
import matplotlib.pyplot as plt
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test , prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test , prediction)
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test , prediction)
#     F1 score
    f1 = f1_score(y_test, prediction)
#     ROC-AUC
    roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy , precision ,recall, f1, roc_auc))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues, normalize=normalize)
            disp.ax_.set_title(title)
        plt.show()


# In[40]:


prediction = grid_xgb.predict(X_test)
get_evaluation(y_test, prediction, grid_xgb, X_test)


# In[41]:


def get_evaluation_by_thresholds(y_test, prediction_proba_class1, thresholds):
    
    for threshold in thresholds:
        binarizer = Binarizer(threshold=threshold).fit(prediction_proba_class1) 
        custom_prediction = binarizer.transform(prediction_proba_class1)
        print('ì„ê³—ê°’:', threshold)
        get_evaluation(y_test, custom_prediction)


# In[43]:


from sklearn.preprocessing import Binarizer
from sklearn.metrics import precision_recall_curve

prediction = grid_xgb.predict(X_test)
prediction_proba_class1 = grid_xgb.predict_proba(X_test)[:, 1].reshape(-1, 1)
precision, recall, thresholds = precision_recall_curve(y_test, prediction_proba_class1)

get_evaluation_by_thresholds(y_test, prediction_proba_class1, thresholds)


# In[45]:


prediction = Binarizer(threshold=0.032460973).fit_transform(prediction_proba_class1)
get_evaluation(y_test, prediction, grid_xgb, X_test)


# ### ë¶€ìŠ¤íŒ…(Boosting) - LightGBM(Light Gradient Boosting Machine)
# 
# #### LGBMClassifier(n_estimators, learning_rate, subsample)
# 
# ##### n_estimators
# - ì•½í•œ í•™ìŠµê¸°ì˜ ê°œìˆ˜ì´ë©°, ê°œìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ì¼ì • ìˆ˜ì¤€ê¹Œì§€ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆì§€ë§Œ ë§ì„ ìˆ˜ë¡ ìˆ˜í–‰ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤.
# - default: 100
# 
# ##### learning_rate
# - GBMì´ í•™ìŠµì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì ìš©í•˜ëŠ” í•™ìŠµë¥ ì´ë‹¤.
# - ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ ë‚˜ê°€ëŠ” ë°ì— ì ìš©í•˜ëŠ” ê³„ìˆ˜ì´ë©°, 0~1ì‚¬ì´ë¡œ ê°’ì„ ì§€ì •í•œë‹¤.
# - ë†’ê²Œ ì„¤ì •í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ê°’ì„ ì°¾ì§€ ëª»í•˜ê³  ì§€ë‚˜ì³ë²„ë¦¬ì§€ë§Œ ë¹ ë¥¸ ìˆ˜í–‰ì´ ê°€ëŠ¥í•˜ê³ ,  
# ë‚®ê²Œ ì„¤ì •í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ ê°’ì„ ì°¾ì•„ì„œ ì„±ëŠ¥ì€ ë†’ì•„ì§€ì§€ë§Œ, ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤.
# <img src="./images/learning_rate.png" width="600" style="margin-left: 0">  
# 
# ##### subsample
# - í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì´ë‹¤.
# - default: 1 (100%)
# - ê³¼ì í•© ë°©ì§€ ì‹œ 1ë³´ë‹¤ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤.
# 
# #### fit(X_train, y_train, eval_set, early_stopping_rounds)
# 
# ##### eval_set
# - ì˜ˆì¸¡ ì˜¤ë¥˜ê°’ì„ ì¤„ì¼ ìˆ˜ ìˆë„ë¡ ë°˜ë³µì í•˜ë©´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ”ë°,   
# ì´ë•Œ í•™ìŠµì€ í•™ìŠµ ë°ì´í„°ë¡œ í•˜ê³  ì˜ˆì¸¡ ì˜¤ë¥˜ê°’ í‰ê°€ëŠ” eval_setë¡œ ì§€ì •ëœ ê²€ì¦ ì„¸íŠ¸ë¡œ í‰ê°€í•œë‹¤.
# 
# ##### early_stopping_rounds
# - ì§€ì •í•œ íšŸìˆ˜ë™ì•ˆ ë” ì´ìƒ ì˜¤ë¥˜ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ë” ì´ìƒ í•™ìŠµì€ ì§„í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

# In[46]:


import lightgbm

print(lightgbm.__version__)


# ##### ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤(COVID) ì˜ˆì¸¡

# In[79]:


import pandas as pd

corona_df = pd.read_csv('./datasets/corona.csv', low_memory=False)
corona_df.info()


# In[80]:


corona_df = corona_df[~corona_df['Cough_symptoms'].isna()]
corona_df = corona_df[~corona_df['Fever'].isna()]
corona_df = corona_df[~corona_df['Sore_throat'].isna()]
corona_df = corona_df[~corona_df['Headache'].isna()]
corona_df['Age_60_above'] = corona_df['Age_60_above'].fillna('No')
corona_df['Sex'] = corona_df['Sex'].fillna('unknown')
corona_df.isna().sum()


# In[81]:


corona_df['Target'] = corona_df['Corona']
corona_df.drop(columns='Corona', axis=1, inplace=True)


# In[82]:


corona_df = corona_df[corona_df['Target'] != 'other']
print(corona_df['Target'].value_counts())


# In[83]:


corona_df = corona_df.drop(columns=['Ind_ID', 'Test_date', 'Sex', 'Age_60_above', 'Known_contact'], axis=1)
corona_df


# In[84]:


corona_df = corona_df.reset_index(drop=True)
corona_df


# In[85]:


corona_df = corona_df.astype('category')
corona_df.info()


# ##### í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ ë° ê²€ì¦

# In[86]:


from sklearn.model_selection import GridSearchCV, train_test_split
from lightgbm import LGBMClassifier

param_grid = {
    'n_estimators': [50, 100, 500],
    'learning_rate': [0.3, 0.5, 0.7]
}

lgbm = LGBMClassifier()

features, targets = corona_df.iloc[:, :-1], corona_df.Target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.2)

# í•™ìŠµ ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬
X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, stratify=y_train, test_size=0.3)

evals = [(X_val_train, y_val_train), (X_val_test, y_val_test)]

grid_lgbm = GridSearchCV(lgbm, param_grid, cv=3, refit=True, return_train_score=True, n_jobs=-1, error_score='raise')
grid_lgbm.fit(X_train, y_train, early_stopping_rounds=50, eval_set=evals)


# In[87]:


# DataFrameìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_lgbm.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']].sort_values(by='rank_test_score')


# In[88]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
import matplotlib.pyplot as plt

# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, prediction_proba_class1, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test, prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test, prediction, pos_label="positive")
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test, prediction, pos_label="positive")
#     F1 score
    f1 = f1_score(y_test, prediction, pos_label="positive")
#     ROC-AUC
    roc_auc = roc_auc_score(y_test, prediction_proba_class1)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy , precision ,recall, f1, roc_auc))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues)
            disp.ax_.set_title(title)
        plt.show()


# In[89]:


prediction = grid_lgbm.predict(X_test)
prediction_proba_class1 = grid_lgbm.predict_proba(X_test)[:, 1].reshape(-1, 1)
get_evaluation(y_test, prediction, prediction_proba_class1)


# In[90]:


from sklearn.preprocessing import Binarizer

def get_evaluation_by_thresholds(y_test, prediction_proba_class1, thresholds):
    for threshold in thresholds:
        binarizer = Binarizer(threshold=threshold).fit(prediction_proba_class1) 
        custom_prediction = binarizer.transform(prediction_proba_class1)
        custom_prediction = custom_prediction.astype('str')
        custom_prediction[custom_prediction == '0.0'] = 'negative'
        custom_prediction[custom_prediction == '1.0'] = 'positive'
        print('ì„ê³—ê°’:', threshold)
        get_evaluation(y_test, custom_prediction, prediction_proba_class1)


# In[91]:


import numpy as np
from sklearn.preprocessing import Binarizer
from sklearn.metrics import precision_recall_curve

prediction_proba_class1 = grid_lgbm.predict_proba(X_test)[:, 1].reshape(-1, 1)

# precision, recall, thresholds = precision_recall_curve(y_test, prediction_proba_class1, pos_label="positive")
thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
get_evaluation_by_thresholds(y_test, prediction_proba_class1, thresholds)


# In[95]:


from sklearn.preprocessing import Binarizer
prediction = Binarizer(threshold=0.1).fit_transform(prediction_proba_class1)
prediction = prediction.astype('str')
prediction[prediction == '0.0'] = 'negative'
prediction[prediction == '1.0'] = 'positive'
get_evaluation(y_test, prediction, prediction_proba_class1)


# ##### permutation_importance

# In[96]:


from sklearn.inspection import permutation_importance

importance = permutation_importance(grid_lgbm, X_test, y_test, n_repeats=100, random_state=0)
corona_df.columns[importance.importances_mean.argsort()[::-1]]

