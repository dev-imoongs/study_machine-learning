#!/usr/bin/env python
# coding: utf-8

# ### π‘€κµμ°¨ κ²€μ¦(Cross Validation)
# - κΈ°μ΅΄ λ°©μ‹μ—μ„λ” λ°μ΄ν„° μ„ΈνΈμ—μ„ ν•™μµ λ°μ΄ν„° μ„ΈνΈμ™€ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ¶„λ¦¬ν• λ’¤ λ¨λΈ κ²€μ¦μ„ μ§„ν–‰ν•λ‹¤.
# - κµμ°¨ κ²€μ¦ μ‹, ν•™μµ λ°μ΄ν„°λ¥Ό λ‹¤μ‹ λ¶„ν• ν•μ—¬ ν•™μµ λ°μ΄ν„°μ™€ λ¨λΈ μ„±λ¥μ„ 1μ°¨ ν‰κ°€ν•λ” κ²€μ¦ λ°μ΄ν„°λ΅ λ‚λλ‹¤.
# <img src="./images/cross_validation01.png" width="500" style="margin-left: -30px">

# ### κµμ°¨ κ²€μ¦μ μ¥λ‹¨μ 
# - π‘νΉμ • λ°μ΄ν„° μ„ΈνΈμ— λ€ν• κ³Όμ ν•© λ°©μ§€
# - π‘λ°μ΄ν„° μ„ΈνΈ κ·λ¨κ°€ μ μ„ μ‹ κ³Όμ†μ ν•© λ°©μ§€
# - π‘λ¨λΈ ν›λ ¨, λ¨λΈ ν‰κ°€μ— μ†μ”λλ” μ‹κ°„ μ¦κ°€  
# β›³ κ³Όμ ν•©μ„ ν”Όν•κ³  ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό νλ‹ν•¨μΌλ΅μ¨ λ¨λΈμ„ μΌλ°ν™”ν•κ³  μ‹ λΆ°μ„±μ„ μ¦κ°€μ‹ν‚¤κΈ° μ„ν•΄μ„ μ‚¬μ©ν•λ‹¤.

# ### κµμ°¨ κ²€μ¦μ μΆ…λ¥
# ##### K-Fold
# - kκ°μ λ°μ΄ν„° ν΄λ“ μ„ΈνΈλ¥Ό λ§λ“  λ’¤ kλ²λ§νΌ ν•™μµκ³Ό κ²€μ¦ ν‰κ°€λ¥Ό λ°λ³µν•μ—¬ μν–‰ν•λ” λ°©μ‹.
# - ν•™μµ λ°μ΄ν„°μ™€ κ²€μ¦ λ°μ΄ν„°λ¥Ό μ •ν™•ν μλ¥΄κΈ° λ•λ¬Έμ— νƒ€κ² λ°μ΄ν„°μ λΉ„μ¤‘μ΄ ν• κ³³μΌλ΅ μΉμ¤‘λ  μ μλ‹¤.
# - μλ¥Ό λ“¤μ–΄, 0, 1, 2, μ¤‘μ—μ„ 0, 1, λ‘ κ°€μ§€λ§ μλΌμ„ κ²€μ¦ν•κ² λλ©΄ λ‹¤λ¥Έ ν•λ‚μ νƒ€κ² λ°μ΄ν„°λ¥Ό μμΈ΅ν•  μ μ—†κ² λλ‹¤.
# - Stratified K-Foldλ΅ ν•΄κ²°ν•λ‹¤.
# ##### Stratified K-Fold
# - K-Foldμ™€ λ§μ°¬κ°€μ§€λ΅ kλ² μν–‰ν•μ§€λ§, ν΄λ“ μ„ΈνΈλ¥Ό λ§λ“¤ λ• ν•™μµ λ°μ΄ν„° μ„ΈνΈμ™€ κ²€μ¦ λ°μ΄ν„° μ„ΈνΈκ°€ κ°€μ§€λ” νƒ€κ² λ¶„ν¬λ„κ°€ μ μ‚¬ν•λ„λ΅ κ²€μ¦ν•λ‹¤.
# - νƒ€κ² λ°μ΄ν„°μ λΉ„μ¤‘μ„ ν•­μƒ λ‘κ°™κ² μλ¥΄κΈ° λ•λ¬Έμ— λ°μ΄ν„°κ°€ ν• κ³³μΌλ΅ μΉμ¤‘λλ” κ²ƒμ„ λ°©μ§€ν•λ‹¤.
# <img src="./images/cross_validation02.png" width="500" style="margin-top:20px; margin-bottom:20px; margin-left: -30px">
# ##### GridSearchCV
# - κµμ°¨ κ²€μ¦κ³Ό μµμ μ ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ„ ν• λ²μ— ν•  μ μλ” κ°μ²΄μ΄λ‹¤.
# - max_depthμ™€ min_samples_splitμ— 1μ°¨μ› μ •μν• listλ¥Ό μ „λ‹¬ν•λ©΄, 2μ°¨μ›μΌλ΅ κ²°ν•©ν•μ—¬ κ²©μ(Grid)λ¥Ό λ§λ“¤κ³ , μ΄ μ¤‘ μµμ μ μ μ„ μ°Ύμ•„λ‚Έλ‹¤.
# - λ”¥λ¬λ‹μ—μ„λ” ν•™μµ μ†λ„κ°€ λ¨Έμ‹ λ¬λ‹μ— λΉ„ν•΄ λλ¦¬κ³ , λ μ΄μ–΄(μΈµ)κ°€ κΉμ–΄μ§ μλ΅ μ΅°μ •ν•΄μ£Όμ–΄μ•Ό ν•  ν•μ΄νΌ νλΌλ―Έν„° κ°’μ΄ λ§μ•„μ§€κΈ° λ•λ¬Έμ—, RandomSearchCVμ—μ„ λ€λµμ μΈ λ²”μ„λ¥Ό μ°Ύμ€ λ‹¤μ, GridSearchCVλ΅ λ””ν…μΌμ„ μ΅°μ •ν•λ” λ°©μ‹μ„ μ‚¬μ©ν•λ‹¤.
# <img src="./images/grid_search_cv.png" width="500" style="margin-top: 20px; margin-left: -30px">

# ##### λ¶“κ½ƒ λ°μ΄ν„°λ΅ κµμ°¨ κ²€μ¦

# In[1]:


from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
features, targets = iris.data, iris.target

target_df = pd.DataFrame(targets, columns=['target'])
target_df.value_counts()


# In[2]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

decision_tree_classifier = DecisionTreeClassifier(min_samples_leaf=6, random_state=124)
kfold = KFold(n_splits=5)


# ##### KFold.split(feature)
# - featuresλ§ μ „λ‹¬ν•κ³  ν•™μµμ©, κ²€μ¦μ© ν–‰ λ²νΈλ¥Ό arrayλ΅ λ¦¬ν„΄ν•λ‹¤.

# In[3]:


for train_index, test_index in kfold.split(features):
    print(train_index)
    print(test_index)
    print("=" * 80)


# In[4]:


count = 0
cv_accuracy = []

for train_index, test_index in kfold.split(features):
    count += 1
    
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = targets[train_index], targets[test_index]
    
    train_targets = pd.DataFrame(y_train)
    test_targets = pd.DataFrame(y_test)
    
    #ν•™μµ λ° μμΈ΅
    decision_tree_classifier.fit(X_train, y_train)
    prediction = decision_tree_classifier.predict(X_test)
    
    # μ •ν™•λ„ μΈ΅μ •
    accuracy = np.round(accuracy_score(y_test, prediction), 4)
    
    cv_accuracy.append(accuracy)
    
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print(f"\n# {count} κµμ°¨ κ²€μ¦ μ •ν™•λ„: {accuracy}, ν•™μµ λ°μ΄ν„° ν¬κΈ°: {train_size}, κ²€μ¦ λ°μ΄ν„° ν¬κΈ°: {test_size}")
    print(f"#{count} ν•™μµ νƒ€κ² λ°μ΄ν„° λ¶„ν¬: \n{train_targets.value_counts()}")
    print(f"#{count} κ²€μ¦ νƒ€κ² λ°μ΄ν„° λ¶„ν¬: \n{test_targets.value_counts()}")
    print(f"#{count} ν•™μµ μ„ΈνΈ μΈλ±μ¤: {train_index}")
    print(f"#{count} κ²€μ¦ μ„ΈνΈ μΈλ±μ¤: {test_index}")
    print("=" * 100)

# ν΄λ“ λ³„ κ²€μ¦ μ •ν™•λ„λ¥Ό ν•©ν•μ—¬ ν‰κ·  μ •ν™•λ„ κ³„μ‚°
print(f"β–¶ ν‰κ·  κ²€μ¦ μ •ν™•λ„: {np.mean(cv_accuracy)}")


# ##### νƒ€κ² λ°μ΄ν„°μ λ¶„ν¬λ¥Ό λ™μΌν•κ² κµμ°¨ κ²€μ¦ μ§„ν–‰
# 
# ##### StratifiedFold.split(features, targets)
# - featuresμ™€ targets λ¨λ‘ μ „λ‹¬ν•κ³  ν•™μµμ©, κ²€μ¦μ© ν–‰ λ²νΈλ¥Ό arrayλ΅ λ¦¬ν„΄ν•λ‹¤.

# In[5]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
import numpy as np
import pandas as pd

decision_tree_classifier = DecisionTreeClassifier(min_samples_leaf=6, random_state=124)

# 5κ°μ ν΄λ“ μ„ΈνΈλ΅ λ¶„λ¦¬
skfold = StratifiedKFold(n_splits=5)


# In[6]:


count = 0
cv_accuracy = []

for train_index, test_index in skfold.split(features, targets):
    count += 1
    
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = targets[train_index], targets[test_index]
    
    train_targets = pd.DataFrame(y_train)
    test_targets = pd.DataFrame(y_test)
    
    #ν•™μµ λ° μμΈ΅
    decision_tree_classifier.fit(X_train, y_train)
    prediction = decision_tree_classifier.predict(X_test)
    
    # μ •ν™•λ„ μΈ΅μ •
    accuracy = np.round(accuracy_score(y_test, prediction), 4)
    
    cv_accuracy.append(accuracy)
    
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print(f"\n# {count} κµμ°¨ κ²€μ¦ μ •ν™•λ„: {accuracy}, ν•™μµ λ°μ΄ν„° ν¬κΈ°: {train_size}, κ²€μ¦ λ°μ΄ν„° ν¬κΈ°: {test_size}")
    print(f"#{count} ν•™μµ νƒ€κ² λ°μ΄ν„° λ¶„ν¬: \n{train_targets.value_counts()}")
    print(f"#{count} κ²€μ¦ νƒ€κ² λ°μ΄ν„° λ¶„ν¬: \n{test_targets.value_counts()}")
    print(f"#{count} ν•™μµ μ„ΈνΈ μΈλ±μ¤: {train_index}")
    print(f"#{count} κ²€μ¦ μ„ΈνΈ μΈλ±μ¤: {test_index}")
    print("=" * 100)

# ν΄λ“ λ³„ κ²€μ¦ μ •ν™•λ„λ¥Ό ν•©ν•μ—¬ ν‰κ·  μ •ν™•λ„ κ³„μ‚°
print(f"β–¶ ν‰κ·  κ²€μ¦ μ •ν™•λ„: {np.mean(cv_accuracy)}")


# ##### νΈν•κ² μν–‰ν•  μ μλ” κµμ°¨ κ²€μ¦
# 
# ##### cross_val_score(estimator, x, y, cv, scoring)
# - estimator: classifier μΆ…λ¥ λ¨λΈμ΄λ©΄ λ‚΄λ¶€μ μΌλ΅ stratified K-Foldλ΅ μ§„ν–‰λλ‹¤.
# - x: featuers
# - y: targets
# - scoring: ν‰κ°€ ν•¨μ, μ •ν™•λ„(accuracy)μ™Έμ— λ‹¤λ¥Έ κ²ƒμ€ λ‹¤λ¥Έ μ¥μ—μ„ λ°°μ΄λ‹¤.
# - cv: ν΄λ“ μ„ΈνΈ κ°μ

# In[7]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris
import numpy as np

iris = load_iris()
decision_tree_classifier = DecisionTreeClassifier(random_state=124, min_samples_leaf=6)

features, targets = iris.data, iris.target

score = cross_val_score(decision_tree_classifier, features, targets, scoring='accuracy', cv=5)
print('κµμ°¨ κ²€μ¦λ³„ μ •ν™•λ„: {}'.format(score))
print('ν‰κ·  μ •ν™•λ„: {}'.format(np.mean(score)))


# #### GridSearchCV(estimator, param_grid, cv, refit, return_train_score)
# - κµμ°¨ κ²€μ¦κ³Ό μµμ  ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ„ ν•λ²μ— ν•  μ μλ‹¤.
# - λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ„ νλ‹ν•κΈ° μ„ν• νλΌλ―Έν„°κ°€ ν•μ΄νΌ νλΌλ―Έν„°.
# - μ΄μ΄ν•κ² νλΌλ―Έν„°λ¥Ό μ…λ ¥ν•λ©΄μ„ ν…μ¤νΈ ν•λ” λ°©μ‹μ΄λ‹¤.
# - λ°μ΄ν„° μ„ΈνΈλ¥Ό cross-validationμ„ μ„ν• ν•™μµ/ν…μ¤νΈ μ„ΈνΈλ΅ μλ™μΌλ΅ λ¶„ν•  ν• λ’¤μ— ν•μ΄νΌ νλΌλ―Έν„° κ·Έλ¦¬λ“μ— κΈ°μ λ λ¨λ“  νλΌλ―Έν„°λ¥Ό μμ°¨μ μΌλ΅ μ μ©ν•μ—¬ μµμ μ νλΌλ―Έν„°λ¥Ό μ°Ύλ”λ‹¤.
# > parameter(νλΌλ―Έν„°)  
#     estimator: ν•™μµν•  λ¨λΈ κ°μ²΄ μ‘μ„±  
#     param_grid: dictν•νƒλ΅ μ „λ‹¬ν•΄μ•Ό ν•λ©°, μ£Όμ” keyκ°’μ€ max_depth, min_samples_splitμ΄λ‹¤.  
#     cv: ν΄λ“ μ„ΈνΈ κ°μ  
#     refit: μµμ μ ν•μ΄νΌ νλΌλ―Έν„°λ΅ μ „λ‹¬ν• λ¨λΈ κ°μ²΄λ¥Ό λ‹¤μ‹ ν›λ ¨ν•κ³ μ ν•  λ• Trueλ¥Ό μ „λ‹¬ν•λ‹¤, λ””ν΄νΈλ” True.  
#     return_train_score: κµμ°¨ κ²€μ¦ μ μλ¥Ό κ°€μ Έμ¬ μ§€μ— λ€ν•΄ True λλ” Falseλ¥Ό μ „λ‹¬ν•λ‹¤.

# In[8]:


from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score

# λ°μ΄ν„°λ¥Ό λ΅λ”©ν•κ³  ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ¥Ό λ¶„λ¦¬ν•λ‹¤.
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=124)
decision_tree_classifier = DecisionTreeClassifier()

# max_depth: λ…Έλ“κ°€ μƒμ„±λλ” μµλ€ κΉμ΄ μ μ ν•
# min_sample_split: μµμ† μƒν” κ°μ μ ν•
parameters = {'max_depth': [2, 3, 4], 'min_samples_split': [6, 7]}


# In[9]:


grid_decision_tree_classifier = GridSearchCV(decision_tree_classifier
                                             , param_grid=parameters
                                             , cv=3
                                             , refit=True
                                             , return_train_score=True)

grid_decision_tree_classifier.fit(X_train, y_train)


# In[10]:


grid_decision_tree_classifier.cv_results_


# In[11]:


import pandas as pd

scores_df = pd.DataFrame(grid_decision_tree_classifier.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']]


# In[12]:


print(f'GridSearchCV μµμ  νλΌλ―Έν„°: {grid_decision_tree_classifier.best_params_}')
print(f'GridSearchCV μµκ³  μ •ν™•λ„: {grid_decision_tree_classifier.best_score_}')

prediction = grid_decision_tree_classifier.predict(X_test)
print(f'ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ μ •ν™•λ„: {accuracy_score(y_test, prediction)}')

# refit λ κ°μ²΄λ” best_estimator_λ΅ κ°€μ Έμ¬ μ μμΌλ©°,
# μ΄λ―Έ grid_decision_tree_classifierκ°μ²΄λ¥Ό GridSearchCVλ΅ μ‘μ—…ν•μ—¬ μƒμ„±ν–κΈ° λ•λ¬Έμ—
# κ²°κ³Όλ” λ‘κ°™μ΄ λ‚μ¨λ‹¤.
estimator = grid_decision_tree_classifier.best_estimator_
prediction = estimator.predict(X_test)
print(f'ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ μ •ν™•λ„: {accuracy_score(y_test, prediction)}')


# In[ ]:





# In[ ]:




