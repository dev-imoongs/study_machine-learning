#!/usr/bin/env python
# coding: utf-8

# ### ë² ì´ì¦ˆ ì¶”ë¡ , ë² ì´ì¦ˆ ì •ë¦¬, ë² ì´ì¦ˆ ì¶”ì •(Bayesian Inference)
# - ì—­í™•ë¥ (inverse probability) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œì„œ, ì¡°ê±´ë¶€ í™•ë¥ (P(B|A))ì„ ì•Œê³  ìˆì„ ë•Œ ì •ë°˜ëŒ€ì¸ ì¡°ê±´ë¶€ í™•ë¥ (P(A|B))ì„ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.
# > ğŸ“Œì¡°ê±´ë¶€ í™•ë¥ (Conditional probability): ë‘ ì‚¬ê±´ A, Bê°€ ìˆì„ ë•Œ, ì‚¬ê±´ Aê°€ ì¼ì–´ë‚¬ì„ ë•Œ Bê°€ ì¼ì–´ë‚  í™•ë¥ ì´ë‹¤.
# <img src="./images/conditional_probability.png" width="200" style="margin-top:20px; margin-left:0">
# - ì¶”ë¡  ëŒ€ìƒì˜ ì‚¬ì „ í™•ë¥ ê³¼ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ëŒ€ìƒì˜ "ì‚¬í›„ í™•ë¥ "ì„ ì¶”ë¡ í•˜ëŠ” í†µê³„ì  ë°©ë²•ì´ë‹¤.  
# ğŸ“Œì‚¬í›„ í™•ë¥ ì´ë€, ì–´ë–¤ ì‚¬ê±´ì´ ë°œìƒí•œ í›„ ì•ìœ¼ë¡œ ì¼ì–´ë‚˜ê²Œ ë  ë‹¤ë¥¸ ì‚¬ê±´ì˜ ê°€ëŠ¥ì„±ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.
# - ì–´ë–¤ ì‚¬ê±´ì´ ì„œë¡œ "ë°°ë°˜"í•˜ëŠ”(ë…ë¦½í•˜ëŠ”) ì›ì¸ ë‘˜ì— ì˜í•´ ì¼ì–´ë‚œë‹¤ê³  í•˜ë©´, ì‹¤ì œ ì‚¬ê±´ì´ ì¼ì–´ë‚¬ì„ ë•Œ ì´ ì‚¬ê±´ì´ ë‘ ì›ì¸ ì¤‘ í•˜ë‚˜ì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ì‹ì´ë‹¤.  
# ğŸ“Œë°°ë°˜í•˜ëŠ” ì›ì¸ì´ë€, í•˜ë‚˜ì˜ ì‚¬ê±´ì´ ì¼ì–´ë‚œ ì›ì¸ì˜ í™•ë¥ ì´ ë‹¤ë¥¸ ì›ì¸ì˜ í™•ë¥ ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³  ê°ê° ë…ë¦½ì ì´ë¼ëŠ” ëœ»ì´ë‹¤.
# - ì–´ë–¤ ìƒí™©ì—ì„œ Nê°œì˜ ì›ì¸ì´ ìˆì„ ë•Œ, ì‹¤ì œ ì‚¬ê±´ì´ ë°œìƒí•˜ë©´ Nê°œ ì¤‘ í•œ ê°€ì§€ ì›ì¸ì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.
# - ê¸°ì¡´ ì‚¬ê±´ë“¤ì˜ í™•ë¥ ì„ ì•Œ ìˆ˜ ì—†ì„ ë•Œ ì „í˜€ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë°©ì‹ì´ë‹¤.  
# í•˜ì§€ë§Œ ê·¸ ê°„ ë°ì´í„°ê°€ ìŒ“ì´ë©´ì„œ, ê¸°ì¡´ ì‚¬ê±´ë“¤ì˜ í™•ë¥ ì„ ëŒ€ëµì ìœ¼ë¡œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.  
# ì´ë¡œ ì¸í•´, ì‚¬íšŒì  í†µê³„ë‚˜ ì£¼ì‹ì—ì„œ ë² ì´ì¦ˆ ì •ë¦¬ í™œìš©ì´ í•„ìˆ˜ë¡œ ê¼½íˆê³  ìˆë‹¤.  
# 
# > ##### ì˜ˆì‹œ
# ì§ˆë³‘ Aì˜ ì–‘ì„±íŒì • ì •í™•ë„ê°€ 80%ì¸ ê²€ì‚¬ê¸°ê°€ ìˆë‹¤. ê²€ì‚¬ë¥¼ ì‹œí–‰í•´ì„œ ì–‘ì„±ì´ ë‚˜ì™”ë‹¤ë©´, ì´ ì‚¬ëŒì´ 80%ì˜ í™•ë¥ ë¡œ ë³‘ì— ê±¸ë ¸ë‹¤ê³  ì´ì•¼ê¸°í•  ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´ ê²€ì‚¬ê¸°ê°€ ì•Œë ¤ì£¼ëŠ” í™•ë¥ ê³¼ ì–‘ì„±ì¼ ê²½ìš° ì§ˆë³‘ì„ ì•“ê³  ìˆì„ í™•ë¥ ì€ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì˜ë¯¸ì—ì„œ ì •ë°˜ëŒ€ì´ê¸° ë•Œë¬¸ì´ë‹¤.  
# <table style="width:50%; margin-left: 50px">
#     <tr>
#         <th>ì „ì œ</th>
#         <th>ê´€ì‹¬ ì‚¬ê±´</th>
#         <th>í™•ë¥ </th>
#     </tr>
#     <tr>
#         <th>ë³‘ì„ ì•“ê³  ìˆë‹¤</th>
#         <th>ì–‘ì„±ì´ë‹¤</th>
#         <th>80%</th>
#     </tr>
#     <tr>
#         <th>ì–‘ì„±ì´ë‹¤</th>
#         <th>ë³‘ì„ ì•“ê³  ìˆë‹¤</th>
#         <th>ì•Œìˆ˜ ì—†ìŒ</th>
#     </tr>
# </table>  
# 
# > ì´ëŸ° ì‹ì˜ í™•ë¥ ì„ êµ¬í•´ì•¼ í•˜ëŠ” ë¬¸ì œë¥¼ ì—­í™•ë¥  ë¬¸ì œë¼ê³  í•˜ê³  ì´ë¥¼ ë² ì´ì¦ˆ ì¶”ë¡ ì„ í™œìš©í•˜ì—¬ êµ¬í•  ìˆ˜ ìˆë‹¤.  
# ë‹¨, ê²€ì‚¬ ëŒ€ìƒì¸ ì§ˆë³‘ì˜ ìœ ë³‘ë¥ (ì‚¬ì „ í™•ë¥ , ê¸°ì¡´ ì‚¬ê±´ë“¤ì˜ í™•ë¥ )ì„ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤.  
# ì „ì„¸ê³„ ì¸êµ¬ ì¤‘ 10%ì˜ ì‚¬ëŒë“¤ì´ ì§ˆë³‘ Aë¥¼ ì•“ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.
# <div style="width: 60%; display:flex; margin-top: -20px; margin-left:30px">
#     <div>
#         <img src="./images/bayesian_inference01.png" width="300" style="margin-top:20px; margin-left:0">
#     </div>
#     <div style="margin-top: 28px; margin-left: 20px">
#         <img src="./images/bayesian_inference02.png" width="310" style="margin-top:20px; margin-left:0">
#     </div>
# </div>  
# 
# <div style="width: 60%; display:flex; margin-left:30px">
#     <div>
#         <img src="./images/bayesian_inference03.png" width="800" style="margin-top:20px; margin-left:0">
#     </div>
#     <div style="margin-top: 28px; margin-left: 20px">
#         <img src="./images/bayesian_inference04.png" width="550" style="margin-top:-8px; margin-left:0">
#     </div>
# </div>  
# 
# > ğŸš©ê²°ê³¼: ì•½ 30.8%
# <img src="./images/bayesian_inference05.png" width="200" style="margin-top:20px; margin-left:0">
# 

# ### ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜(Naive Bayes Classifier)
# - í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•´ ì „í†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë¶„ë¥˜ê¸°ë¡œì„œ, ë¶„ë¥˜ì— ìˆì–´ì„œ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
# - ë² ì´ì¦ˆ ì •ë¦¬ì— ê¸°ë°˜í•œ í†µê³„ì  ë¶„ë¥˜ ê¸°ë²•ìœ¼ë¡œì„œ, ì •í™•ì„±ë„ ë†’ê³  ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ëŒ€í•œ ì†ë„ë„ ë¹ ë¥´ë‹¤.
# - ë°˜ë“œì‹œ ëª¨ë“  featureê°€ ì„œë¡œ ë…ë¦½(independent)ì ì´ì–´ì•¼ í•œë‹¤. ì¦‰, ì„œë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” featureë“¤ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•œë‹¤.
# - ê°ì • ë¶„ì„, ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì¶”ì²œ ì‹œìŠ¤í…œ ë“± ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ í™œìš©ë˜ëŠ” ë¶„ë¥˜ ê¸°ë²•ì´ë‹¤.
# - ë¹ ë¥´ê³  ì •í™•í•˜ê³  ê°„ë‹¨í•œ ë¶„ë¥˜ ë°©ë²•ì´ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ì—ì„œ ëª¨ë“  featureê°€ ë…ë¦½ì ì¸ ê²½ìš°ëŠ” ë“œë¬¼ê¸° ë•Œë¬¸ì— ì‹¤ìƒí™œì— ì ìš©í•˜ê¸° í˜ë“¤ë‹¤.
# <img src="./images/naive_bayes_classifier.png" width="400" style="margin-left: 0">

# ### ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì¢…ë¥˜
# ##### BernoulliNB(ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ)  
# - ê°€ì¥ ê¸°ë³¸ì ì¸ NB í•¨ìˆ˜ë¡œ ì´ì§„ ë¶„ë¥˜ì— ì‚¬ìš©í•œë‹¤.
# ##### CategoricalNB  
# - ë¶„ë¥˜í•  ì¹´í…Œê³ ë¦¬ì˜ ì¢…ë¥˜ê°€ 3ê°€ì§€ ì´ìƒì¼ ë•Œ ì‚¬ìš©í•œë‹¤.
# ##### MultinomialNB (ë©€íƒ€ì´ë…¸ìš°ë¯¸ì–¼(ë‹¤í•­) ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ) 
# - í…ìŠ¤íŠ¸ì˜ ë“±ì¥ íšŸìˆ˜ì²˜ëŸ¼ ì´ì‚°ì ì¸ ê°’ì˜ ìˆ˜ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì‚¬ìš©í•œë‹¤.
# ##### GaussianNB (ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ) 
# - ì˜ˆì¸¡í•  ê°’ì´ ì—°ì†ì ì¸ ê°’ì¸ ê²½ìš°ì— ì‚¬ìš©í•œë‹¤.
# ##### ComplementNB  
# - target labelì˜ balanceê°€ ë§ì§€ ì•ŠëŠ” ë¶ˆê· í˜•í•œ ìƒí™©ì— ì‚¬ìš©í•œë‹¤.

# ### ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜

# In[1]:


import pandas as pd

mail_df = pd.read_csv('./datasets/spam.csv')
mail_df


# In[2]:


mail_df.info()


# ### ë ˆì´ë¸” ì¸ì½”ë”©

# In[3]:


from sklearn.preprocessing import LabelEncoder

mail_encoder = LabelEncoder()
targets = mail_encoder.fit_transform(mail_df.Category)
mail_df['Category'] = targets


# In[4]:


mail_encoder.classes_


# In[5]:


from sklearn.feature_extraction.text import CountVectorizer

text = ["í–„ë²„ê±°ëŠ” ë§›ìˆì–´! ì •ë§ ë§›ìˆì–´", 
        "ì•„ë‹ˆì•¼ í”¼ìê°€ ë” ë§›ìˆì–´ í–„ë²„ê±°ë³´ë‹¤ ë” ë§›ìˆì–´!",
        "ì•„ë‹ˆì•¼ ë‘˜ ë‹¤ ë¨¹ì!"]
        
count_vec = CountVectorizer()
m = count_vec.fit_transform(text)
print(m.toarray())

# ê° ì—´ë²ˆí˜¸ê°€ ëœ»í•˜ëŠ” ë‹¨ì–´
print(count_vec.vocabulary_)


# In[6]:


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(mail_df.Message, mail_df.Category, test_size=0.3, stratify=mail_df.Category)


# In[7]:


from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

navie_bayes_pipeline = Pipeline([('count_vectorizer', CountVectorizer()), ('naive_bayes', MultinomialNB())])
navie_bayes_pipeline.fit(X_train, y_train)


# In[8]:


prediction = navie_bayes_pipeline.predict(X_test)

# ìŠ¤íŒ¸ ë©”ì¼ë¡œ íŒë‹¨í•œ ì´ë©”ì¼ì„ ì •ë ¬ì„ í†µí•´ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
prediction[prediction == 1].argsort()
# í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ 1(ìŠ¤íŒ¸)ì¸ì§€ ê²€ì‚¬
print(f'ì˜ˆì¸¡ ê²°ê³¼: {prediction[216]}')

# featureì—ì„œ ë™ì¼í•œ ì¸ë±ìŠ¤ì˜ ë©”ì„¸ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°
# ì‹¤ì œ ì •ë‹µì´ 1(ìŠ¤íŒ¸)ì¸ì§€ ê²€ì‚¬
print('ì‹¤ì œ ì •ë‹µ: {}'.format(mail_df[mail_df['Message'] == X_test.to_list()[216]].Category.to_list()[0]))


# In[9]:


navie_bayes_pipeline.score(X_test, y_test)


# In[10]:


from sklearn.metrics import accuracy_score

accuracy_score(y_test, prediction)


# In[11]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
import matplotlib.pyplot as plt
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test , prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test , prediction)
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test , prediction)
#     F1 score
    f1 = f1_score(y_test, prediction)
#     ROC-AUC
    roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy , precision ,recall, f1, roc_auc))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues, normalize=normalize)
            disp.ax_.set_title(title)
        plt.show()


# In[14]:


get_evaluation(y_test, prediction, navie_bayes_pipeline, X_test)


# In[ ]:




