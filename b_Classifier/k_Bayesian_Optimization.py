#!/usr/bin/env python
# coding: utf-8

# ### ë² ì´ì§€ì•ˆ ìµœì í™”(Bayesian Optimization)
# ##### ì°¸ê³ : https://www.youtube.com/watch?app=desktop&v=w9D8ozS0oC4
# <img src="./images/bayesian_optimization.png" width="700" style="margin-left: -20px">  
# 
# - ìµœì†Œì˜ ì‹œë„ë¡œ ìµœì ì˜ ë‹µì„ ì°¾ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•˜ë©°, ê°œë³„ ì‹œë„ì— ìˆì–´ì„œ ë§ì€ ì‹œê°„ ë° ìì›ì´ í•„ìš”í•  ë•Œë„ ì‚¬ìš©í•œë‹¤.
# - ë¯¸ì§€ì˜ í•¨ìˆ˜ê°€ ë¦¬í„´í•˜ëŠ” ê°’ì˜ ìµœì†Œ ë˜ëŠ” ìµœëŒ€ê°’ì„ ë§Œë“œëŠ” ìµœì ì˜ í•´ë¥¼ ì§§ì€ ë°˜ë³µì„ í†µí•´ ì°¾ì•„ë‚´ëŠ” ìµœì í™” ë°©ì‹ì´ë‹¤.
# - ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•˜ì„ ë•Œ ìµœì  í•¨ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ê°œì„ í•´ ë‚˜ê°€ë©´ì„œ ìµœì  í•¨ìˆ˜ë¥¼ ë„ì¶œí•œë‹¤.
# - ëŒ€ì²´ ëª¨ë¸ê³¼ íšë“ í•¨ìˆ˜ë¡œ êµ¬ì„±ë˜ë©°, ëŒ€ì²´ ëª¨ë¸ì€ íšë“ í•¨ìˆ˜ë¡œë¶€í„° ìµœì  ì…ë ¥ ê°’ì„ ì¶”ì²œë°›ì€ ë’¤ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ í•´ë‚˜ê°„ë‹¤.  
# íšë“ í•¨ìˆ˜ëŠ” ê°œì„ ëœ ëŒ€ì²´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ìµœì  ì…ë ¥ ê°’ì„ ê³„ì‚°í•œë‹¤.
# - í•¨ìˆ˜ì˜ ê³µë¶„ì‚°(covariance)ì´ í¬ë‹¤ëŠ” ê²ƒì€ ê³§ ë¶ˆí™•ì‹¤ì„±ì´ í¬ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ê³µë¶„ì‚°ì´ ìµœëŒ€ì¸ ì§€ì ì„ ë‹¤ìŒ ìƒ˜í”Œë§ í¬ì¸íŠ¸ë¡œ ì„ ì •í•œë‹¤.
# - ğŸ“Œê³µë¶„ì‚°(Cov)ì´ë€, 2ê°œì˜ í™•ë¥ ë³€ìˆ˜ì˜ ì„ í˜• ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ìœ¼ë¡œì„œ, ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ ì‚¬ì´ì— ì–¼ë§ˆë‚˜ ì˜ì¡´í•˜ëŠ”ì§€ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‘œí˜„í•œë‹¤.
# 
# ---
# <div style="display: flex">
#     <div>
#         <p style="width: 90%; text-align:center">ê³µë¶„ì‚°ì´ ì–‘ìˆ˜ì¼ ê²½ìš°</p>
#         <img src="./images/covariance01.png" width="700" style="margin-left: -30px">
#     </div>
#     <div>
#         <p style="width: 90%; text-align:center">ê³µë¶„ì‚°ì´ ìŒìˆ˜ì¼ ê²½ìš°</p>
#         <img src="./images/covariance02.png" width="700" style="margin-left: -30px">
#     </div>
#     <div>
#         <p style="width: 90%; text-align:center">ê³µë¶„ì‚°ì´ 0ì¼ ê²½ìš°</p>
#         <img src="./images/covariance03.png" width="700" style="margin-left: -30px">
#     </div>
# </div>  
#   
# ##### ğŸš©ê³µë¶„ì‚°ì´ í°, x = 2ì¸ ì§€ì ì— ìƒ˜í”Œë§ì„ í•˜ë©´, ë¶ˆí™•ì‹¤ì„±ì´ ê°ì†Œí•˜ê²Œ ëœë‹¤.
# <div style="display: flex">
#     <div>
#         <img src="./images/bayesian01.png" width="500" style="margin-left: -40px; margin-bottom: 20px">
#     </div>
#     <div>
#         <img src="./images/bayesian02.png" width="465" style="margin-left: -30px">
#     </div>
# </div>  
#   
# ##### ğŸš©ê³µë¶„ì‚°ì´ í°, x = -0.5ì¸ ì§€ì ì— ìƒ˜í”Œë§ì„ í•˜ë©´, ë¶ˆí™•ì‹¤ì„±ì´ ê°ì†Œí•˜ê²Œ ëœë‹¤.
# <div style="display: flex">
#     <div>
#         <img src="./images/bayesian02.png" width="460" style="margin-left: -20px; margin-bottom: 20px">
#     </div>
#     <div>
#         <img src="./images/bayesian03.png" width="455" style="margin-left: -10px">
#     </div>
# </div>  
# 

# In[1]:


import xgboost

print(xgboost.__version__)


# In[2]:


import pandas as pd

corona_df = pd.read_csv('./datasets/corona.csv', low_memory=False)
corona_df.info()


# In[3]:


corona_df = corona_df[~corona_df['Cough_symptoms'].isna()]
corona_df = corona_df[~corona_df['Fever'].isna()]
corona_df = corona_df[~corona_df['Sore_throat'].isna()]
corona_df = corona_df[~corona_df['Headache'].isna()]
corona_df.isna().sum()


# In[4]:


corona_df['Target'] = corona_df['Corona']
corona_df.drop(columns='Corona', axis=1, inplace=True)
corona_df


# In[5]:


corona_df = corona_df[corona_df['Target'] != 'other']
corona_df['Target'].value_counts()


# In[6]:


corona_df = corona_df.drop(columns=['Ind_ID', 'Test_date', 'Sex', 'Known_contact', 'Age_60_above'], axis=1)
corona_df


# In[7]:


from sklearn.preprocessing import LabelEncoder

columns = ['Fever', 'Sore_throat', 'Shortness_of_breath', 'Headache', 'Cough_symptoms', 'Target']

for column in columns:
    encoder = LabelEncoder()
    targets = encoder.fit_transform(corona_df[column])
    corona_df.loc[:, column] = targets
    print(f'{column}_classes: {encoder.classes_}')


# In[8]:


# ê° ì¹´í…Œê³ ë¦¬ ê°’ì„ ì •ìˆ˜ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ !
corona_df = corona_df.astype('int16')
corona_df.info()


# In[9]:


from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

features, targets = corona_df.iloc[:, :-1], corona_df.Target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.3)

# ì˜¤ë²„ ìƒ˜í”Œë§
# ê²€ì¦ ë°ì´í„°ë‚˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì•„ë‹Œ í•™ìŠµë°ì´í„°ì—ì„œë§Œ ì˜¤ë²„ìƒ˜í”Œë§ ì‚¬ìš©í•  ê²ƒ
smote = SMOTE(random_state=0)
X_train_over, y_train_over = smote.fit_resample(X_train, y_train)

print('SMOTE ì ìš© ì „:\n',pd.Series(y_train).value_counts() )
print('SMOTE ì ìš© í›„:\n',pd.Series(y_train_over).value_counts() )

# í•™ìŠµ ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬
X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, stratify=y_train, test_size=0.5)

evals = [(X_val_train, y_val_train), (X_val_test, y_val_test)]


# In[10]:


from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score
import matplotlib.pyplot as plt
# íƒ€ê²Ÿ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.
def get_evaluation(y_test, prediction, classifier=None, X_test=None):
#     ì˜¤ì°¨ í–‰ë ¬
    confusion = confusion_matrix(y_test, prediction)
#     ì •í™•ë„
    accuracy = accuracy_score(y_test, prediction)
#     ì •ë°€ë„
    precision = precision_score(y_test, prediction)
#     ì¬í˜„ìœ¨
    recall = recall_score(y_test, prediction)
#     F1 score
    f1 = f1_score(y_test, prediction)
#     ROC-AUC
    roc_auc = roc_auc_score(y_test, prediction)

    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy , precision ,recall, f1, roc_auc))
    print("#" * 75)
    
    if classifier is not None and  X_test is not None:
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
        titles_options = [("Confusion matrix", None), ("Normalized confusion matrix", "true")]

        for (title, normalize), ax in zip(titles_options, axes.flatten()):
            disp = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues)
            disp.ax_.set_title(title)
        plt.show()


# In[11]:


from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
# ëª©ì  í•¨ìˆ˜ ì •ì˜
def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ ì •ì˜
    n_estimators = trial.suggest_int('n_estimators', 100, 1000, step=100)
    max_depth = trial.suggest_int('max_depth', 5, 15)
    
    model = LGBMClassifier(
        boost_from_average=False,
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=124
    )
    
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train_over, y_train_over, early_stopping_rounds=50, eval_set=evals)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
    prediction = model.predict(X_test)
    
    # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€
    get_evaluation(y_test, prediction)
    auc = roc_auc_score(y_test, prediction)
    return auc


# In[12]:


# conda install -c conda-forge optuna
import optuna
import time

start_time = time.time()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print("optuna ìˆ˜í–‰ ì‹œê°„: {0:.1f} ì´ˆ ".format(time.time() - start_time))


# In[13]:


best_params = study.best_params
best_score = study.best_value

print('Best Parameters:', best_params)
print('Best Score:', best_score)


# In[14]:


model = LGBMClassifier(
        boost_from_average=False,
        n_estimators=1000,
        max_depth=12,
        random_state=124
    )

model.fit(X_train_over, y_train_over, early_stopping_rounds=50, eval_set=evals)

prediction = model.predict(X_test)

get_evaluation(y_test, prediction, model, X_test)

