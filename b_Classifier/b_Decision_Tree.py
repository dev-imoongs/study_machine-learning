#!/usr/bin/env python
# coding: utf-8

# ### ğŸŒ³ê²°ì • íŠ¸ë¦¬(Decision Tree)
# ##### ê·¸ë¦¼ ì¶œì²˜: ì½”ë”©í•˜ëŠ” ìˆ˜í•™ìŒ¤, ìŠ¤íŒŸ
# - ë§¤ìš° ì‰½ê³  ìœ ì—°í•˜ê²Œ ì ìš©ë  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œì„œ ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë§, ì •ê·œí™” ë“±ì˜ ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì˜ì¡´ë„ê°€ ë§¤ìš° ì ë‹¤.
# - í•™ìŠµì„ í†µí•´ ë°ì´í„°ì— ìˆëŠ” ê·œì¹™ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ë‚´ì–´ Tree ê¸°ë°˜ì˜ ë¶„ë¥˜ ê·œì¹™ì„ ë§Œë“ ë‹¤.
# - if-else ê¸°ë°˜ì˜ ê·œì¹™ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë¶„ë¥˜ì˜ ê¸°ì¤€ì„ ì •í•˜ëŠ” ê²ƒì´ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ì— ë§ì€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
# - ì˜í–¥ì„ ê°€ì¥ ë§ì´ ë¯¸ì¹˜ëŠ” featureë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ë„ ìˆë‹¤.
# - ğŸ‘ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ë³µì¡í•œ ê·œì¹™ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•˜ê¸° ë•Œë¬¸ì— ê³¼ì í•©(Overfitting)ì´ ë°œìƒí•´ì„œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ë„ ìˆë‹¤.  
#   ë”°ë¼ì„œ íŠ¸ë¦¬ì˜ í¬ê¸°ë¥¼ ì‚¬ì „ì— ì œí•œí•˜ëŠ” íŠœë‹ì„ í•„ìš”ë¡œ í•œë‹¤.
# <img src="./images/decision_tree01.jpg" width="450" style="margin-top:20px; margin-bottom:20px; margin-left: -30px">  
# - ê°€ì¥ ìƒìœ„ ë…¸ë“œë¥¼ "ë£¨íŠ¸ ë…¸ë“œ"ë¼ê³  í•˜ë©°, ë‚˜ë¨¸ì§€ ë¶„ê¸°ì ì„ "ì„œë¸Œ ë…¸ë“œ", ê²°ì •ëœ ë¶„ë¥˜ê°’ ë…¸ë“œë¥¼ "ë¦¬í”„ ë…¸ë“œ"ë¼ê³  í•œë‹¤.
# <img src="./images/decision_tree02.png" width="450" style="margin-top:20px; margin-bottom:20px; margin-left: -30px">  
# - ë³µì¡ë„ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ê²ƒì´ ì£¼ëª©ì ì´ë©°, ì •ë³´ì˜ ë³µì¡ë„ë¥¼ ë¶ˆìˆœë„(Impurity)ë¼ê³  í•˜ë©°, ì´ë¥¼ ìˆ˜ì¹˜í™”í•œ ê°’ìœ¼ë¡œëŠ” ì§€ë‹ˆ ê³„ìˆ˜(Gini coefiicient), ì—”íŠ¸ë¡œí”¼(Entropy)ê°€ ìˆë‹¤.  
# - 1. í´ë˜ìŠ¤ê°€ ì„ì´ì§€ ì•Šê³  ë¶„ë¥˜ê°€ ì˜ ë˜ì—ˆë‹¤ë©´, ë¶ˆìˆœë„ê°€ ë‚®ë‹¤. 
# - 2. í´ë˜ìŠ¤ê°€ ì„ì—¬ ìˆê³ , ë¶„ë¥˜ê°€ ì˜ ì•ˆë˜ì—ˆë‹¤ë©´, ë¶ˆìˆœë„ê°€ ë†’ë‹¤.
# <img src="./images/impurity.png" width="350" style="margin-top:20px; margin-left: -30px">
# 
# > ##### Gini coefiicient [ì§€ë‹ˆ ì½”ìš°ì–´í”¼ì…˜íŠ¸]  
# <img src="./images/gini01.png" width="160" style="margin-left: -10px">
# > - ë¹¨ê°„ ê³µ 6ê°œì™€ íŒŒë€ê³µ 4ê°œì¼ ê²½ìš° G = 0.48
# <img src="./images/gini02.png" width="280" style="margin-left: -10px">  
# 
# > - í†µê³„ì  ë¶„ì‚° ì •ë„ë¥¼ ì •ëŸ‰í™”í•˜ì—¬ í‘œí˜„í•œ ê°’ì´ê³ , 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ë‹¤.
# > - ì§€ë‹ˆ ê³„ìˆ˜ê°€ ë‚®ì„ ìˆ˜ë¡ ë¶„ë¥˜ê°€ ì˜ ëœ ê²ƒì´ë‹¤.
# > - Decision Tree Model  
# > CART (Classification and Regression Tree): ë‹¨ ë‘ê°œì˜ ë…¸ë“œ(Binary Tree)
# 
# ---
# > ##### Entropy
# <img src="./images/entropy01.png" width="200" style="margin-left: 0">
# > - ë¹¨ê°„ ê³µ 6ê°œì™€ íŒŒë€ê³µ 4ê°œì¼ ê²½ìš° E = 0.97
# <img src="./images/entropy02.png" width="250" style="margin-left: 0">
# <img src="./images/entropy03.png" width="500" style="margin-left: 0">  
# 
# > - í™•ë¥ ë¶„í¬ê°€ ê°€ì§€ëŠ” ì •ë³´ì˜ í™•ì‹ ë„ í˜¹ì€ ì •ë³´ëŸ‰(ì •ë³´ì˜ ì–‘)ì„ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤.
# > - ì—”íŠ¸ë¡œí”¼ ê°’ì´ ì‘ì„ ìˆ˜ë¡ ë¶„ë¥˜ê°€ ì˜ ëœ ê²ƒì´ë‹¤.
# > - Decision Tree Model  
# > ID3(Iterative Dichotomiser 3): ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ë²”ì£¼í˜• ë°ì´í„°ì¸ ê²½ìš°ë§Œ ì‚¬ìš© ê°€ëŠ¥  
# C4.5 (ID3 ì•Œê³ ë¦¬ì¦˜ì˜ ê²°ì ì„ ë³´ì™„): ì—¬ëŸ¬ ê°œì˜ ìì‹ ë…¸ë“œ  
# > ğŸ“Œë²”ì£¼í˜• ë°ì´í„°: 'A', 'B', 'C'ì™€ ê°™ì´ ì¢…ë¥˜ë¥¼ í‘œì‹œí•˜ëŠ” ë°ì´í„°(ì¹´í…Œê³ ë¦¬ ë°ì´í„°)
# 
# ---
# > ##### Misclassification Rate
# > - ì˜ëª» ì˜ˆì¸¡ëœ ê´€ì¸¡ì¹˜ì˜ ë°±ë¶„ìœ¨
# 
# 
# ##### ğŸ“Œê³¼ì í•©(Overfitting)
# - í•™ìŠµ ë°ì´í„°ë¥¼ ê³¼í•˜ê²Œ ì˜ í•™ìŠµì‹œì¼œì„œ ì˜¤ì°¨ê°€ ì˜¤íˆë ¤ ì¦ê°€í•˜ëŠ” í˜„ìƒì´ë‹¤.
# - ì˜ˆë¥¼ ë“¤ì–´, ë¹¨ê°„ìƒ‰ ì‚¬ê³¼ì˜ featureë¥¼ ê³¼í•˜ê²Œ í•™ìŠµì‹œí‚¤ë©´, ì´ˆë¡ìƒ‰ ì‚¬ê³¼ë¥¼ ì‚¬ê³¼ë¡œ ì¸ì‹í•˜ì§€ ëª»í•˜ê²Œ ëœë‹¤.
# <img src="./images/overfitting.png" width="350" style="margin-left: -30px">
# - í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì˜¤ì°¨ê°€ ê°ì†Œí•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì˜¤ì°¨ê°€ ì¦ê°€í•œë‹¤.

# ### Graphviz
# - ê²°ì •íŠ¸ë¦¬ ëª¨ë¸ì„ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤.
# - https://graphviz.org/download/, graphviz-9.0.0 (64-bit) EXE installer [sha256]
# - https://drive.google.com/file/d/1oCXidIjNAvUT2UcNFEdhRfFhnZ96iHrp/view?usp=sharing

# In[2]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# DecisionTreeClassifier ìƒì„±
decision_tree_classifier = DecisionTreeClassifier(random_state=124)

# ë¶“ê½ƒ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³ , í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¡œ ë¶„ë¦¬
iris = load_iris()

X_train, X_test, y_train, y_test = \
train_test_split(iris.data, iris.target, test_size=0.2, random_state=13)

# DecisionTreeClassifier í•™ìŠµ
decision_tree_classifier.fit(X_train, y_train)


# In[13]:


iris


# In[3]:


from sklearn.tree import export_graphviz

# export_graphviz()ì˜ out_fileë¡œ ì§€ì •ëœ iris_tree01.dot íŒŒì¼ì„ ìƒì„±
export_graphviz(decision_tree_classifier
                , out_file="./images/iris_tree01.dot"
                , class_names=iris.target_names
                , feature_names=iris.feature_names
                , impurity=True
                , filled=True)


# In[4]:


import graphviz

with open("./images/iris_tree01.dot") as f:
    dot_graph = f.read()

iris_tree01_graph = graphviz.Source(dot_graph)


# In[5]:


iris_tree01_graph.render(filename="iris_tree01", directory='./images', format="png")


# <img src="./images/iris_tree01.png" width=800 style="margin-left:0">

# ### featureë³„ ì¤‘ìš”ë„
# ê° featureê°€ ë¶„ë¥˜ë¥¼ ë‚˜ëˆ„ëŠ” ë°ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ë¥¼ í‘œê¸°í•˜ëŠ” ì²™ë„ì´ë‹¤.
# ##### feature_importances_
# - ë¶„ë¥˜ë¥¼ ê²°ì •í•˜ëŠ” ë°ì— ê°€ì¥ ì¤‘ìš”ë„ê°€ ë†’ì•˜ë˜ featureë¥¼ ë¦¬í„´í•œë‹¤.

# In[6]:


for i in zip([1,2,3], [9,8,7]):
    print(i)


# In[7]:


for name, value in zip(iris.feature_names, decision_tree_classifier.feature_importances_):
    print(f'{name}, {round(value, 4)}')


# In[8]:


import seaborn as sns

sns.barplot(x=decision_tree_classifier.feature_importances_, y=iris.feature_names)


# ### ê²°ì • íŠ¸ë¦¬ì˜ ê³¼ì í•©
# - ìœ„ì—ì„œ ì•Œì•„ë‚¸ ê²ƒì²˜ëŸ¼ petal length, petal width, 2ê°œì˜ featureë§Œìœ¼ë¡œë„ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì—, 2ì°¨ì› ì‚°ì ë„(ë¶„í¬ë„)ë¥¼ í†µí•´ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤.
# - ë§ˆì§€ë§‰ê¹Œì§€ ë…¸ë“œê°€ ë¶„ë¦¬ë˜ì—ˆê¸° ë•Œë¬¸ì— ê³¼ì í•©ì´ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆê³ , ì´ë¥¼ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤.  
# ğŸ“Œí•˜ì´í¼ íŒŒë¼ë¯¸í„°ë€, ìµœì ì˜ í›ˆë ¨ ëª¨ë¸ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ì¹˜ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.

# In[18]:


import matplotlib.pyplot as plt

features, targets = iris.data, iris.target

plt.title("3 Targets with 2 Features")
plt.scatter(features[:, 2], features[:, 3], marker='o', c=targets, s=25, cmap="rainbow", edgecolors='k')


# In[20]:


import numpy as np

# Classifierì˜ Decision Boundaryë¥¼ ì‹œê°í™” í•˜ëŠ” í•¨ìˆ˜
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
    
    # í•™ìŠµ ë°ì´íƒ€ scatter plotìœ¼ë¡œ ë‚˜íƒ€ë‚´ê¸°
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # í˜¸ì¶œ íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ì˜¨ training ë°ì´íƒ€ë¡œ model í•™ìŠµ . 
    model.fit(X, y)
    # meshgrid í˜•íƒœì¸ ëª¨ë“  ì¢Œí‘œê°’ìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() ë¥¼ ì´ìš©í•˜ì—¬ class boundary ë¥¼ visualization ìˆ˜í–‰. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow',
                           zorder=1)


# In[21]:


from sklearn.tree import DecisionTreeClassifier

features, targets = iris.data[:, [2, 3]], iris.target

decision_tree_classifier = DecisionTreeClassifier(random_state=124).fit(features, targets)
visualize_boundary(decision_tree_classifier, features, targets)


# ##### ìµœì†Œ smaplesì˜ ê°œìˆ˜ë¥¼ 6ê°œë¡œ ì œí•œí•œë‹¤.

# In[22]:


from sklearn.tree import DecisionTreeClassifier

features, targets = iris.data[:, [2, 3]], iris.target

decision_tree_classifier = DecisionTreeClassifier(random_state=124, min_samples_leaf=6).fit(features, targets)
visualize_boundary(decision_tree_classifier, features, targets)


# In[32]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# DecisionTreeClassifier ìƒì„±
decision_tree_classifier = DecisionTreeClassifier(random_state=124, min_samples_leaf=6)

# ë¶“ê½ƒ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³ , í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¡œ ë¶„ë¦¬
iris = load_iris()

X_train, X_test, y_train, y_test = \
train_test_split(iris.data, iris.target, test_size=0.2, random_state=13)

# DecisionTreeClassifier í•™ìŠµ
decision_tree_classifier.fit(X_train, y_train)


# In[33]:


from sklearn.tree import export_graphviz

# export_graphviz()ì˜ out_fileë¡œ ì§€ì •ëœ iris_tree02.dot íŒŒì¼ì„ ìƒì„±
export_graphviz(decision_tree_classifier
                , out_file="./images/iris_tree02.dot"
                , class_names=iris.target_names
                , feature_names=iris.feature_names
                , impurity=True
                , filled=True)


# In[34]:


import graphviz

with open("./images/iris_tree02.dot") as f:
    dot_graph = f.read()

iris_tree01_graph = graphviz.Source(dot_graph)


# In[35]:


iris_tree01_graph.render(filename="iris_tree02", directory='./images', format="png")


# <img src="./images/iris_tree02.png" width=800 style="margin-left:0">

# ##### make_classification()
# - ë¶„ë¥˜ìš© ê°€ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤.
# - n_features : ë…ë¦½ ë³€ìˆ˜ì˜ ìˆ˜, ë””í´íŠ¸ 20
# - n_redundant : ë…ë¦½ ë³€ìˆ˜ ì¤‘ ë‹¤ë¥¸ ë…ë¦½ ë³€ìˆ˜ì˜ ì„ í˜• ì¡°í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì„±ë¶„ì˜ ìˆ˜, ë””í´íŠ¸ 2
# - n_informative : ë…ë¦½ ë³€ìˆ˜ ì¤‘ ì¢…ì† ë³€ìˆ˜ì™€ ìƒê´€ ê´€ê³„ê°€ ìˆëŠ” ì„±ë¶„ì˜ ìˆ˜, ë””í´íŠ¸ 2
# - n_clusters_per_class : í´ë˜ìŠ¤ ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜, ë””í´íŠ¸ 2

# In[36]:


from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

plt.title("3 Targets with 2 Features")

# 2ì°¨ì› ì‹œê°í™”ë¥¼ ìœ„í•´ì„œ featureëŠ” 2ê°œ, targetì€ 3ê°€ì§€ ìœ í˜•ì˜ classification ìƒ˜í”Œ ë°ì´í„° ìƒì„±.
X_features, y_targets = make_classification(n_features=2, n_redundant=0, n_classes=3, n_clusters_per_class=1, random_state=124)

plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_targets, s=25, cmap="rainbow", edgecolors='k')


# In[37]:


from sklearn.tree import DecisionTreeClassifier

decision_tree_classifier = DecisionTreeClassifier(random_state=124).fit(X_features, y_targets)
visualize_boundary(decision_tree_classifier, X_features, y_targets)


# In[38]:


from sklearn.tree import DecisionTreeClassifier

decision_tree_classifier = DecisionTreeClassifier(random_state=124, min_samples_leaf=6).fit(X_features, y_targets)
visualize_boundary(decision_tree_classifier, X_features, y_targets)


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




